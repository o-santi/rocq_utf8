#import "template.typ": project, hex

#set raw(syntaxes: "Coq.sublime-syntax")

#show: project.with(
    title: "Verifica√ß√£o formal de uma implementa√ß√£o eficiente de um decodificador de UTF-8",
    authors: ((
        name: "Leonardo Santiago",
        email: "leonardors@dcc.ufrj.br",
        affiliation: "UFRJ",
    ),),
    abstract: [O sistema de codifica√ß√£o #emph("Unicode") √© imprescind√≠vel para a comunica√ß√£o global, permitindo que in√∫meras linguagens utilizem a mesma representa√ß√£o para transmitir todas os caracteres, eliminando a necessidade de convers√£o. Dentre todos os formatos de serializar caracteres do Unicode - denominados #emph("codepoints") - certamente o formato mais ub√≠quito √© o UTF-8, pela sua retro compatibilidade com ASCII, e a capacidade de economizar bytes. Apesar de ser utilizado em mais de 98% das p√°ginas da internet, v√°rios problemas aparecem ao implementar programas de codifica√ß√£o e decodifica√ß√£os de UTF-8 sem√¢nticamente corretos, e in√∫meras vulnerabilidades est√£o associadas a esse processo. Dificultando ainda mais, a especifica√ß√£o dada pelo Cons√≥rcio Unicode √© feita inteiramente em prosa, tornando extremamente dif√≠cil afirmar com seguran√ßa que dada implementa√ß√£o respeita-a por m√©todos tradicionais. Assim, este trabalho utilizar√° verifica√ß√£o formal atrav√©s de provadores de teoremas interativos com dois prop√≥sitos. Primeiro, ser√° desenvolvido um conjunto de propriedades - a especifica√ß√£o - que unicamente representam um par de programas codificador e decodificador de UTF-8. Com a especifica√ß√£o formalizada, ser√£o implementados um codificador e decodificador, mostrando que esses respeitam todas as propriedades necess√°rias para que estejam corretos. ]
)

= Introdu√ß√£o
// https://speakerdeck.com/alblue/a-brief-history-of-unicode?slide=7
// https://www.unwoundstack.com/blog/testing-poor-substitute-for-reasoning.html
// https://www.unwoundstack.com/blog/type-theory.html
// https://vladris.com/blog/2018/11/18/notes-on-encoding-text.html
// https://tonsky.me/blog/unicode/
// https://en.wikipedia.org/wiki/Han_unification

O processo de desenvolvimento de software pode ser separado em duas fases distintas: a de valida√ß√£o, que pretende desenvolver especifica√ß√µes necess√°rias para que um programa resolva um problema no mundo real, e a de verifica√ß√£o, que assegura que o programa desenvolvido implementa essas especifica√ß√µes.

Especifica√ß√£o √© o principal t√≥pico de estudo das pr√°ticas de modelagem de software, que tem como produ√ß√£o gr√°ficos conceituais, modelos e regras de neg√≥cio, que devem ser utilizados para desenvolver o programa. O objetivo dessas √© gerar um conjunto de objetivos e propriedades que programas devem satisfazer para que atinjam algum fim no mundo real, conferindo sem√¢ntica √† resultados e implementa√ß√µes, e construindo pontes tang√≠veis entre modelos te√≥ricos e a realidade pr√°tica.

Assegurar que dada implementa√ß√£o segue as regras de neg√≥cio geradas na fase de especifica√ß√£o √© t√≥pico de estudo da √°rea de verifica√ß√£o. Dela, in√∫meras pr√°ticas comuns na √°rea de programa√ß√£o s√£o derivadas, como desenvolvimento de testes, garantias de qualidade e checagens de tipo. Apesar das in√∫meras pr√°ticas, preencher a lacuna entre a sem√¢ntica dos modelos te√≥ricos e as implementa√ß√µes em c√≥digo √© extremamente dif√≠cil, dada a natureza das pr√°ticas tradicionais baseadas em testes unit√°rios. Testes oferecem vis√µes circunstanciais do comportamento do programa a partir de certas condi√ß√µes iniciais, tornando imposs√≠vel assegurar com totalidade a corretude do programa, visto que programas complexos teriam de ter um n√∫mero impratic√°vel de testes  -- muitas vezes infinito -- para checar todas as combina√ß√µes de condi√ß√µes iniciais.

√â cotidiano que erros passem desapercebidos por baterias gigantescas de testes e apare√ßam somente em produ√ß√£o -- quando erros s√£o inaceit√°veis -- em especial quando ocorrem em combina√ß√µes muito espec√≠ficas de entrada. Muitas linguagens ent√£o tomam uma abordagem din√¢mica, isto √©, tornar erros mais f√°ceis de serem detectados adicionando in√∫meras checagens enquanto o programa executa, e tornando-o programa ainda mais sucet√≠vel a erros. Para atingir _software_ correto, √© imprescind√≠vel a an√°lise est√°tica dos programas, mas t√©cnicas comuns de an√°lise est√°tica n√£o s√£o potentes o suficiente para conferir seguran√ßa e corretude, e s√£o mais complexas do que abordagens din√¢micas.

Verifica√ß√£o formal de software denomina a √°rea da verifica√ß√£o que oferece diretrizes para raciocinar formalmente sobre um programa, descrevendo axiomas, regras e pr√°ticas que permitem construir provas sobre o comportamento desse. Ao estruturar o programa para permitir o racioc√≠nio matem√°tico, torna-se poss√≠vel atribuir uma sem√¢ntica a um software, conferindo fortes garantias de corretude, e assegurando-se que esse est√° conforme as especifica√ß√µes da sem√¢ntica. Para auxiliar nesse processo, v√°rias ferramentas foram desenvolvidas, como _model checkers_, que tentam gerar provas automaticamente a partir de modelos fornecidos, e provadores de teorema interativos, que permitem o desenvolvedor de elaborar provas sobre programas utilizando linguagens espec√≠ficas para constru√≠-las.

Por necessitar que programas sejam estruturados de maneira a facilitar o racioc√≠nio l√≥gico, a metodologia da verifica√ß√£o formal dificilmente √© aplicada a projetos complexos j√° existentes, visto que tradicionalmente s√£o feitos com outros objetivos em mente -- facilidade de desenvolvimento, agilidade em desenvolver novas capacidades, ou at√© mesmo velocidade do programa gerado. Al√©m disso, as ferramentas mais poderosas de verifica√ß√£o formal, os provadores de teoremas interativos, utilizam tipos dependentes, que nativamente utilizam linguagens funcionais para sua l√≥gica interna, o que significa que expressar programas imperativos nessas geralmente requer muito mais trabalho. Assim, fica claro que existem certas barreiras para a ado√ß√£o de m√©todos formais na ind√∫stria.

O objetivo deste trabalho √©, portanto, documentar os benef√≠cios, bem como as dificuldades, da aplica√ß√£o desses m√©todos a problemas suficientemente complexos, de forma a confirmar ou refutar o estigma existente na ado√ß√£o da verifica√ß√£o formal. Em particular, o problema da codifica√ß√£o e decodifica√ß√£o de caracteres em UTF-8 fora escolhido pela sua difus√£o em praticamente todos os contextos e linguagens de programa√ß√£o.

O padr√£o Unicode (@unicode) de representa√ß√£o de caracteres √© ub√≠quito na comunica√ß√£o na internet, e seu principal formato de codifica√ß√£o e decodifica√ß√£o, UTF-8, √© utilizado em mais de 98% das p√°ginas web (@Utf8Usage2025). Apesar disso, in√∫meras CVEs est√£o associadas a programas que tratam UTF-8 incorretamente, especialmente por n√£o implementarem totalmente a especifica√ß√£o, visto que muitos casos incomuns podem acabar sendo esquecidos.

As vulnerabilidades CVE-2000-0884 (Microsoft IIS) e CVE-2008-2938 (APACHE Tomcat) est√£o diretamente associadas √† m√° gest√£o de input ao ler caracteres UTF-8, permitindo ao atacante de ler arquivos em caminhos fora do inicialmente permitido (ataque conhecido como _directory traversal_). A CVE-2004-2579 (Novell iChain) est√° associada a um ataque que utiliza representa√ß√µes ilegais de caracteres de escape em UTF-8 para ultrapassar regras de controle. A CVE-2007-6284 (libxml2) permite que ataques de nega√ß√£o de servi√ßo (/loops/ infinito) atrav√©s da utiliza√ß√£o de caracteres mal formados em textos XML.

// https://github.com/JuliaStrings/utf8proc/tree/master

// https://discourse.julialang.org/t/bug-in-isvalid-with-an-overlong-utf-8-encoded-vector-or-string/15290 & https://github.com/JuliaLang/julia/issues/11141

// https://github.com/python/cpython/blob/da7f4e4b22020cfc6c5b5918756e454ef281848d/Parser/tokenizer/helpers.c#L447

// https://unicodebook.readthedocs.io/issues.html#non-strict-utf-8-decoder-overlong-byte-sequences-and-surrogates

// https://www.cve.org/CVERecord?id=CVE-2007-6284

// https://github.com/bminor/glibc/blob/91fb9914d867320d65a2abe284fb623d91ae5efb/iconvdata/tst-table-from.c#L110 fun√ß√£o na glibc que aceita utf8 de at√© 6 caracteres + overlongs.

// https://unicodebook.readthedocs.io/programming_languages.html#c-language

N√£o apenas programas espec√≠ficos est√£o sujeitos a erros na implementa√ß√£o, mas at√© mesmo implementa√ß√µes b√°sicas em linguagens difundidas cometem erros cruciais. O leitor de UTF-8 da linguagem PHP em vers√µes mais antigas n√£o tratava corretamente casos especiais desse sistema, tornando poss√≠vel inje√ß√µes de SQL (CVE-2009-4142), _cross site scripting_ (CVE-2010-3870), e _integer overflows_ (CVE-2009-5016). At√© mesmo a linguagem Julia, criada em 2012 -- anos depois da consolida√ß√£o do sistema Unicode -- apresentou problemas de decodifica√ß√£o de sequ√™ncias inv√°lidas UTF-8. Dessa forma, fica claro que a formaliza√ß√£o formal como forma de assegurar corretude e seguran√ßa √© uma ferramenta valiosa.

Este trabalho √© estruturado nas seguintes se√ß√µes:
1. Na se√ß√£o 2, a hist√≥ria por tr√°s do sistema Unicode ser√° revista, com o objetivo de motivar a estrutura√ß√£o atual dos sistemas de codifica√ß√£o UTF-8, UTF-16 e UTF-32, bem como algumas de suas propriedades e  limita√ß√µes. Tamb√©m ser√° inspecionada a literatura existente, tanto especifica√ß√µes existentes do Unicode quanto sobre abordagens e metodologias tradicionais de provar formalmente a corretude de codificadores e decodificadores de linguagens.
3. Na se√ß√£o 3, ser√° elaborado um conjunto de regras formais que um codificador e decodificador, denominado de *especifica√ß√£o*, e ser√£o provados teoremas que fundamentam a corretude desse. Para auxiliar na prova de teoremas, √© desenvolvida uma teoria sobre isomorfismos em conjuntos finitos.
4. Na se√ß√£o 4, ser√£o desenvolvidos implementa√ß√µes pr√°ticas de um codificador e decodificador UTF-8, levando em considera√ß√£o fatores como simplicidade, utilidade e efici√™ncia, de maneira similar a como s√£o implementados em linguagens "imperativas".
5. Na se√ß√£o 5, ser√£o dadas as considera√ß√µes finais, bem como aplica√ß√µes naturais desse trabalho para cen√°rios pr√°ticos.

Neste trabalho est√£o contidas as seguintes contribui√ß√µes:

1. A primeira prova formal de que h√° um mapeamento √∫nico entre o formato oficial de bytes UTF-8 e codepoints v√°lidos, isto √©, que a especifica√ß√£o do Unicode est√° correta.
2. Um conjunto de regras formais para decidir automaticamente se dado codificador ou decodificador respeita o formato UTF-8, junto de provas de corretude sobre esse conjunto de regras, de forma a motivar sua relev√¢ncia. Em especial, √© utilizada uma abordagem inovadora utilizando fun√ß√µes crescentes para completamente descrever a codifica√ß√£o UTF-8.
3. Uma implementa√ß√£o formalmente correta, no sentido das regras supracitadas, de tanto um codificador quanto decodificador.

#pagebreak()

= Unicode

// https://tonsky.me/blog/unicode/
// https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses

Sistemas de codifica√ß√£o s√£o padr√µes criados para transformar caracteres em n√∫meros, como `A`=$65$, `√É`=$195$, `Ë™û`=$35486$ e ü§™=$129322$, e posteriormente serializ√°-los em mensagens para envi√°-los a outras pessoas. O padr√£o Unicode √© o sistema de representa√ß√£o de caracteres mais utilizado mundialmente hoje em dia, por incluir todas as linguagens existentes de maneira integrada. O padr√£o define 3 esquemas de codifica√ß√£o distintos para transformar caracteres Unicode em sequ√™ncias de bits, UTF-8, UTF-16 e UTF-32, sendo UTF-8 de longe formato mais utilizado. Para entender o design e funcionamento desses, faz-se necess√°rio entender como funcionavam os antecessores.

#quote(block: true, [Defini√ß√£o: _*code point*_ (ou *valor escalar*) √© o nome dado √† representa√ß√£o num√©rica de um caractere. No formato Unicode, √© comum represent√°-los no formato `U+ABCDEF`, onde `ABCDEF` armazena o n√∫mero do _code point_ em hexadecimal. ])

#quote(block: true, [Defini√ß√£o: um *codificador* √© um programa que recebe valores escalares e transforma-os sequ√™ncias de bits, e um *decodificador* √© um programa que le sequ√™ncias de bits e transforma-os de volta em valores escalares. ])

Sem d√∫vidas o sistema de codifica√ß√£o mais influente da hist√≥ria √© o ASCII. Criado para servir as necessidades da ind√∫stria americana de _teleprinters_, o ASCII define apenas 127 caracteres, focando principalmente em compactar a quantidade de bits necess√°rios para enviar uma mensagem, de forma que todo caracter pode ser expresso utilizando apenas 7 bits.

Com a evolu√ß√£o dos computadores, e a consolida√ß√£o de um byte como 8 bits, muitos sistemas de codifica√ß√£o surgiram mantendo os primeiros 127 caracteres iguais a ASCII, e adicionando 128 caracteres no final, utilizando o oitavo bit previamente ignorado. Esses foram criados primariamente para adicionar suporte √† caracteres especificos de cada linguagem, como `√É`, `√ß`, e `‚Ç¨`, de modo a manter compatibilidade com o ASCII, e ficaram conhecidos como codifica√ß√µes de ASCII estendido.

Tanto o ASCII quanto suas extens√µes utilizam um mapeamento um pra um entre o n√∫mero dos caracteres e os bits das suas representa√ß√µes, tanto por simplicidade de codifica√ß√£o quanto por efici√™ncia de armazenamento de mem√≥ria. Programas que decodificam bytes em caracteres nesses sistemas s√£o extremamente simples, e podem ser resumidos a tabelas de convers√£o direta, conhecidas como _code pages_.

Apesar da simplicidade dos programas, representar um byte por caractere coloca uma severa limita√ß√£o no n√∫mero de caracteres que conseguem expressar ($<= 256$), fazendo com que cada linguagem diferente tivesse sua pr√≥pria maneira distinta de representar seus caracteres, e que muitas vezes era incompat√≠vel com as outras. Assim, enviar textos pela internet era uma tarefa complicada, visto que n√£o era garantido que o usu√°rio que recebe a mensagem teria as tabelas necess√°rias para decodific√°-la corretamente.

Para piorar a situa√ß√£o, linguagens baseadas em ideogramas, como japon√™s, coreano e chin√™s, possuem milhares de caracteres, e codific√°-las em apenas um byte √© imposs√≠vel. Tais linguagens foram pioneiras em encodings multi-bytes, em que um caractere √© transformado em mais de um byte, tornando a codifica√ß√£o e decodifica√ß√£o significativamente mais complexa.

O padr√£o Unicode fora criado ent√£o para que um √∫nico sistema de codifica√ß√£o pudesse cobrir todas as linguagens, com todos seus caracteres espec√≠ficos, de forma que qualquer texto escrito em qualquer linguagem pudesse ser escrito nele. Apesar de ambicioso, esse sistema rapidamente ganhou ado√ß√£o massiva, estabelecendo sua posi√ß√£o como principal m√©todo de codifica√ß√£o da internet.

== UCS-2 e UTF-16

#let r(t) = text(fill: red, t)
#let g(t) = text(fill: green, t)
#let b(t) = text(fill: blue, t)
#let o(t) = text(fill: orange, t)
#let p(t) = text(fill: purple, t)
#let m(t) = text(fill: maroon, t)

Em 1991, a vers√£o 1.0 do Unicode fora lan√ßado pelo cons√≥rcio Unicode, com uma codifica√ß√£o de tamanho fixo de 16 bits conhecida por UCS-2 -- _Universal Coding System_ -- capaz de representar 65536 caracteres das mais diversas l√≠nguas. Rapidamente, esse sistema ganhou ado√ß√£o em sistemas de grande relev√¢ncia, como o sistema de UI Qt (1992), Windows NT 3.1 (1993) e at√© mesmo linguagens como Java (1995).

Tal quantidade, apesar de muito maior do que os antigos 256, rapidamente provou-se n√£o suficiente para todas as linguagens. Quando isso fora percebido, o sistema UCS-2 j√° estava em amplo uso, e troc√°-lo por outro sistema j√° n√£o era mais uma tarefa trivial. Assim, para estend√™-lo mantendo-o retro compat√≠vel, decidiram reservar parte da tabela de caracteres para que dois caracteres distintos (32 bits) representem um √∫nico _code point_. Dessa forma, o sistema deixou de ter um tamanho fixo de 16 bits, e passou a ter um tamanho vari√°vel, dependendo de quais _code points_ s√£o codificados.

// https://en.wikipedia.org/wiki/UTF-16
O padr√£o UCS-2 estendido com _surrogate pairs_ tornou-se oficialmente o padr√£o UTF-16 na vers√£o 2.0 do Unicode. Desde ent√£o, o uso do UCS-2 √© desencorajado, visto que UTF-16 √© considerado uma extens√£o em todos os aspectos a ele. Hoje em dia, na vers√£o 17.0 do padr√£o Unicode, 297,334 _code points_ j√° foram definidos, muito al√©m da proje√ß√£o inicial de 65536.

Para determinar se uma sequ√™ncia de bytes √© v√°lida em UTF-16, faz se necess√°rio determinar se o primeiro byte representa o in√≠cio de um _surrogate pair_, representado por bytes entre `D800` e `DBFF`, seguido de bytes que representam o fim de um _surrogate pair_, entre `DC00` e `DFFF`. Considerando a tabela 3.5 oferecida no cap√≠tulo 3.9 da especifica√ß√£o Unicode, o esquema de serializa√ß√£o pode ser visto da seguinte forma:
#let gr(t) = text(fill: gray, t)
#figure(
    align(center, table(columns: (auto, auto, auto, auto),
        align: (right, right, right, right),
        stroke: none,
        table.header("Valor escalar", table.cell(align:center, "Bytes"), table.cell(colspan: 2, align:center, "Inicio..Fim")),
        r(`xxxxxxxxxxxxxxxx`),  [#r(`xxxxxxxx`) #r(`xxxxxxxx`)], `U+0000`, `U+FFFF`,
        [#gr(`000`)#b(`uuuuu`)#r(`xxxxxxxxxxxxxxxx`)], [`110110`#g(`ww`) #g(`ww`)#r(`xxxxxx`) `110111`#r(`xx`) #r(`xxxxxxxx`)], `U+10000`, `U+10FFFF`,
    )), caption: [Distribui√ß√£o dos bits em bytes v√°lidos UTF-16. Nota: #g(`wwww`)` = `#b(`uuuuu`)` - 1`])

Assim, para que a decodifica√ß√£o de UTF-16 seja n√£o amb√≠gua, √© necess√°rio que _code points_ do primeiro intervalo, que n√£o possuem cabe√ßalho para diferenci√°-los, n√£o possam come√ßar com a sequ√™ncia de bits `11011`. Al√©m disso, iniciar um _surrogate pair_ (`D800..DBFF`) e n√£o termin√°-lo com um _code point_ no intervalo correto (`DC00..DFFF`) √© considerado um erro, e √© inv√°lido segundo a especifica√ß√£o. De fato, o padr√£o Unicode explicita que *nenhum* _code point_ pode ser representado pelo intervalo `U+D800..U+DFFF`, de forma que os outros sistemas de codifica√ß√£o -- UTF-8, UTF-32 -- tenham que desenvolver sistemas para evitar que esses sejam considerados _code points_ v√°lidos.

A quantidade de _code points_ definidos pelo Unicode est√° diretamente ligada √† essas limita√ß√µes do padr√£o UTF-16, que consegue expressar $1.112.064$ _code points_. Esse n√∫mero pode ser calculado da seguinte forma:
#figure(align(center, table(columns: (auto, auto, auto),
    stroke: none,
    table.header("Inicio..Fim", "Tamanho", "Descri√ß√£o"),
    `U+0000..U+FFFF`, $2^16$, "Basic Multilingual Plane, Plane 0",
    `U+D800..U+DFFF`, $2^11$, "Surrogate Pairs",
    `U+10000..U+10FFFF`, $2^20$, "Higher Planes, Planes 1-16",
    table.hline(), 
    [`U+0000..U+10FFFF` #sym.without `U+D800..U+DFFF`], $2^20 + 2^16 - 2^11$, [_Code points_ represent√°veis]
)), caption: [Intervalos de _code points_ v√°lidos.])

Disso, pode-se inferir que um _code point_ *v√°lido* √© um n√∫mero de 21 bits que:
1. N√£o est√° no intervalo `U+D800..U+DFFF`.
2. N√£o ultrapassa `U+10FFFF`.

// https://nvd.nist.gov/vuln/detail/CVE-2008-2938
// https://nvd.nist.gov/vuln/detail/CVE-2012-2135

√â importante ressaltar que h√° ambiguidade na forma de serializar UTF-16 para bytes, visto que n√£o √© especificado pelo Unicode se o primeiro byte de um _code point_ deve ser o mais significativo -- Big Endian -- ou o menos significativo -- Little Endian. Para distinguir, √© comum o uso do caractere `U+FEFF`, conhecido como _Byte Order Mark_ (BOM), como o primeiro caractere de uma mensagem ou arquivo. No caso de Big Endian, o BOM aparece como `FEFF`, e no caso de Little Endian, aparece como `FFFE`.

Essa distin√ß√£o faz com que UTF-16 seja divido em duas sub linguagens, UTF-16BE (Big Endian) e UTF-16LE (Little Endian), adicionando ainda mais complexidade √† tarefa de codificar e decodificar os caracteres corretamente.

Com essas complexidades, implementar codifica√ß√£o e decodifica√ß√£o de UTF-16 corretamente tornou-se muito mais complicado. Determinar se uma sequ√™ncia de bytes deixou de ser uma tarefa trivial, e tornou-se um poss√≠vel lugar onde erros de seguran√ßa podem acontecer. De fato, CVE-2008-2938 e CVE-2012-2135 s√£o exemplos de vulnerabilidades encontradas em fun√ß√µes relacionadas √† decodifica√ß√£o em UTF-16, em projetos grandes e bem estabelecidas (APACHE e Python, respectivamente).

Apesar de extremamente √∫til, o UTF-16 utiliza 2 bytes para cada caractere, ent√£o n√£o √© eficiente para linguagens cujos caracteres encontram-se no intervalo original do ASCII (1 byte por caractere). Em especial formatos comumente utilizados na internet, como HTML e JSON, usam muitos caracteres de pontua√ß√£o -- `<`, `>`, `{`, `:` -- contidos no intervalo do ASCII. Por isso, fez-se necess√°rio achar outra forma de codific√°-los que fosse mais eficiente para a comunica√ß√£o digital.

== UTF-8

Criado por Rob Pike e Ken Thompson, o UTF-8 surgiu como uma alternativa ao UTF-16 que utiliza menos bytes. A principal mudan√ßa para que isso fosse poss√≠vel foi a de abandonar a ideia de codifica√ß√£o de tamanho fixo desde o in√≠cio, dificultando ainda mais a implementa√ß√£o de decodificadores corretos, e preferindo uma codifica√ß√£o de tamanho vari√°vel.

A quantidade de bytes necess√°rios para representar um _code point_ em UTF-8 √© uma fun√ß√£o do intervalo que esse _code point_ se encontra. Ao inv√©s de serializar os _code points_ diretamente, como o UTF-16 fazia, agora todos os bytes cont√©m cabe√ßalhos, que indicam o tamanho da serializa√ß√£o do _code point_ -- isto √©, a quantidade de bytes a seguir.

Para _code points_ no intervalo `U+0000..U+007F`, apenas 1 byte √© usado, e esse deve come√ßar com o bit `0`. Para _code points_ no intervalo `U+0080..07FF`, dois bytes s√£o usados, o primeiro come√ßando com os bits `110`, e o segundo sendo um byte de continua√ß√£o, que cont√©m o cabe√ßalho `10`. Para aqueles no intervalo `U+0800..U+FFFF`, o primeiro byte deve come√ßar com `1110`, seguido de dois bytes de continua√ß√£o, e por fim, aqueles no intervalo `U+10000..U+10FFFF`, o primeiro byte deve come√ßar com `11110`, seguido de tr√™s bytes de continua√ß√£o.

Considerando a tabela 3.6 do cap√≠tulo 3.9 da especifica√ß√£o, podemos representar como os bytes serializados com a seguinte tabela:
#figure(align(center, table(columns: (auto, auto, auto, auto, auto, auto, auto),
    align: (right, right, right, right, right, right, left),
    stroke: none,
    table.header(table.cell(align:center, "Bits do Valor Escalar"), table.cell(colspan:4, align:center, "Bytes"), table.cell(colspan: 2, align:center, "In√≠cio..Fim")),
    [#gr(`00000`) #gr(`00000000`) #gr(`0`)#r(`xxxxxxx`)], [], [], [], [`0`#r(`xxxxxxx`)], `U+0000`, `U+007F`,
    [#gr(`00000`) #gr(`00000`)#b(`yyy`) #b(`yy`)#r(`xxxxxx`)], [], [], [`110`#b(`yyyyy`)], [`10`#r(`xxxxxx`)], `U+0080`, `U+07FF`,
    [#gr(`00000`) #o(`zzzz`)#b(`yyyy`) #b(`yy`)#r(`xxxxxx`)], [], [`1110`#o(`zzzz`)], [`10`#b(`yyyyyy`)], [`10`#r(`xxxxxx`)], `U+0800`, `U+FFFF`,
    [#p(`uuuuu`) #o(`zzzz`)#b(`yyyy`) #b(`yy`)#r(`xxxxxx`)], [`11110`#p(`uuu`)], [`10`#p(`uu`)#o(`zzzz`)], [`10`#b(`yyyyyy`)], [`10`#r(`xxxxxx`)], `U+10000`, `U+10FFFF`,
)), caption: [Distribui√ß√£o dos bits em bytes UTF-8.]) <UTF8_bits>

√â importante notar que os primeiros 127 _code points_ s√£o representados exatamente igual caracteres ASCII em apenas 1 byte, algo extremamente desej√°vel n√£o apenas para compatibilidade com sistemas antigos, mas para recuperar parte da efici√™ncia de espa√ßo perdida no UTF-16. Diferentemente do UTF-16, o UTF-8 tamb√©m n√£o possui ambiguidade de _endianness_, e portanto n√£o precisa utilizar o BOM para desambiguar; h√° apenas uma maneira de ordenar os bytes.

O UTF-8 ainda precisa manter as limita√ß√µes do UTF-16, visto que ambos codificam o mesmo conjunto de _code points_. Como _surrogate pairs_ n√£o s√£o mais utilizados para representar _code points_ estendidos, √© necess√°rio garantir que bytes do intervalo `D800..DFFF` nunca apare√ßam, j√° que n√£o possuem significado.

Al√©m disso, apesar de conseguir codificar 21 bits no caso com maior capacidade (`U+0000..U+1FFFFF`), nem todos desses representam _code points_ v√°lidos, dado que o padr√£o Unicode define-os baseando nos limites do UTF-16. Isso significa que o codificador deve assegurar de que todos _code points_ decodificados n√£o sejam maior do que `U+10FFFF`.

As primeiras vers√µes da especifica√ß√£o do UTF-8 n√£o faziam distin√ß√£o de qual o tamanho deveria ser utilizado para codificar um _code point_. Por exemplo, o caractere `A` √© representado por `U+0041 = `#r(`1000001`). Isso significa que ele podia ser representado em UTF-8 como qualquer uma das seguintes sequ√™ncias:

#let gr(t) = text(fill: gray, t)
#figure(align(center, table(columns: (auto, auto),
    align: (right, left),
    stroke: none,
    table.header("Sequ√™ncia de bits", "Hexadecimal"),
    [`0`#r(`1000001`)], `41`,
    [`110`#gr(`0000`)#r(`1`) `10`#r(`000001`)], `C1 81`,
    [`1110`#gr(`0000`) `10`#gr(`00000`)#r(`1`) `10`#r(`000001`)], `E0 81 81`,
    [`11110`#gr(`000`) `10`#gr(`000000`) `10`#gr(`00000`)#r(`1`) `10`#r(`000001`)], `F0 80 81 81`,
)), caption: [Poss√≠veis representa√ß√µes para o caracter `U+0041`.])

// https://www.cve.org/CVERecord?id=CVE-2010-3870
// https://kevinboone.me/overlong.html

Permitir tais codifica√ß√µes causou in√∫meras vulnerabilidades de seguran√ßa, visto que v√°rios programas erroneamente ignoram a no√ß√£o de _code points_ e tratam esses como sequ√™ncias de bytes diretamente. Ao tentar proibir certos caracteres de aparecerem em uma string, os programas procuravam por sequ√™ncias de bytes especificamente, ao inv√©s de _code points_, e ignoravam que um _code point_ podia ser codificado de outra forma. As CVEs CVE-2008-2938 e CVE-2000-0884 est√£o associadas diretamente com m√° filtragem de caracteres em strings, permitindo que o atacante codifique caracteres proibidos com diferentes sequ√™ncias de bytes (`/`, `..`) e ultrapasse todas as checagens.

O padr√£o Unicode nomeou esses casos como _overlong encodings_, e modificou especifica√ß√µes futuras para que a √∫nica codifica√ß√£o v√°lida de um _code point_ em UTF-8 seja a menor poss√≠vel. Isso adiciona ainda mais dificuldade na hora de decodificar os bytes, visto que o conte√∫do do _code point_ deve ser observado, para checar se fora codificado do tamanho certo.

Assim, validar que uma sequ√™ncia de bytes representa UTF-8 v√°lido significa respeitar as seguintes propriedades:
1. Nenhum byte est√° no intervalo de _code points_ de _surrogate pairs_ (`U+D800..U+DFFF`), e consequentemente, nenhum _code point_ deve ocupar esse intervalo tamb√©m.
2. Todo _code point_ lido √© menor ou igual a `U+10FFFF`
3. Todo _code point_ √© escrito na menor quantidade de bytes necess√°ria para express√°-lo, isto √©, n√£o h√° _overlong encoding_.
4. Todo byte de in√≠cio come√ßa com o header correto (a depender do intervalo do _codepoint_).
5. Todo byte de continua√ß√£o come√ßa com o header correto (`10`).

== Revis√£o de literatura

// https://www.swift.org/blog/utf8-string/
// https://github.com/rust-lang/rust/blob/master/library/core/src/str/validations.rs#L126
// TODO: figure out whatever the hack swift does for UTF-8 validation:
// https://github.com/swiftlang/swift/blob/89b43dccf31d5331cd7fe1336d44e6407e08eadc/stdlib/public/core/UTF8.swift#L14

A proposi√ß√£o original do sistema de codifica√ß√£o UTF-8 fora dada no RFC3629, que passou por m√∫ltiplas revis√µes, at√© ser oficialmente transferida para a especifica√ß√£o Unicode, a partir de sua vers√£o 4.0, em 2003. Desde ent√£o, a defini√ß√£o autorit√°ria para esse esquema √© dada pelo Cons√≥rcio Unicode, dentro da especifica√ß√£o geral do sistema Unicode.

No cap√≠tulo 3.9 da especifica√ß√£o do sistema Unicode, s√£o definidos conceitos gerais de codifica√ß√£o, bem como os formatos UTF-8, UTF-16 e UTF-32. Nesse cap√≠tulo, duas defini√ß√µes importantes s√£o feitas:

1. [D77] *Valor escalar*: um valor escalar Unicode √© qualquer code point que n√£o est√° no intervalo de _surrogate pairs_. (Esse defini√ß√£o √© a mesma de code points v√°lidos dada anteriormente.)
2. [D79] *Esquema de codifica√ß√£o Unicode*: um mapeamento √∫nico entre um valor escalar e uma sequ√™ncia de bytes. A especifica√ß√£o oferece a defini√ß√£o de tr√™s esquemas de codifica√ß√£o oficiais: UTF-32 ([D90]), UTF-16 ([D91]) e UTF-8 ([D92]). 

Segundo a defini√ß√£o D92, o UTF-8 √© um formato de codifica√ß√£o que transforma um escalar Unicode em uma sequ√™ncia de 1 a 4 bytes, cujos bits representam code points exatamente como especifiado na @UTF8_bits. Para decidir quais bytes s√£o UTF-8 v√°lidos, √© oferecida a tabela 3.7, reproduzida abaixo em verbatim:

#figure(align(center, table(columns: (auto, auto, auto, auto, auto, auto),
    align: (right, right, left, right, right, right),
    stroke: none,
    table.header(table.cell(colspan: 2, align:center, "In√≠cio..Fim"), table.cell(align:center, "Byte 1"), table.cell(align:center, "Byte 2"), table.cell(align:center, "Byte 3"), table.cell(align:center, "Byte 4")),
    table.hline(),
    [`U+0000`], [`U+007F`], [`00..7F`],       none, none, none,
    table.hline(stroke: (thickness: 0.5pt, dash:"dashed")),
    [`U+0080`], [`U+07FF`], [`C2..DF`], [`80..BF`], none, none,
    table.hline(stroke: (thickness: 0.5pt, dash:"dashed")),
    [`U+0800`], [`U+0FFF`], [`E0`], [*`A0`*`..BF`], [`80..BF`], none,
    [`U+1000`], [`U+CFFF`], [`E1..EC`], [`80..BF`], [`80..BF`], none,
    [`U+D000`], [`U+D7FF`], [`ED`], [`80..`*`9F`*], [`80..BF`], none,
    [`U+E000`], [`U+FFFF`], [`EE..EF`], [`80..BF`], [`80..BF`], none,
    table.hline(stroke: (thickness: 0.5pt, dash:"dashed")),
    [`U+10000`], [`U+3FFFF`], [`F0`], [*`90`*`..BF`], [`80..BF`], [`80..BF`],
    [`U+40000`], [`U+FFFFF`], [`F1..F3`], [`80..BF`], [`80..BF`], [`80..BF`],
    [`U+100000`], [`U+10FFFF`], [`F4`], [`80..`*`8F`*], [`80..BF`], [`80..BF`],
)), caption: [Sequ√™ncias de bytes UTF-8 bem formadas.]) <UTF8_bytes>

Os intervalos `80..BF` representam os intervalos comuns de continua√ß√£o -- isto √©, bytes que come√ßam com `10` sempre est√£o nesse intervalo -- e portanto, os bytes que diferem desses est√£o marcados em negrito. Essas diferen√ßas s√£o necess√°rias para evitar os casos de _overlong encoding_ -- onde o _code point_ representado caberia em uma representa√ß√£o menor -- e de _surrogate pair_ -- onde o _code point_ representado estaria no intervalo `D800..DFFF`.

No caso em que o _code point_ est√° no intervalo ASCII, ele √© representado sem restri√ß√µes. Quando √© necess√°rio dois bytes, o primeiro n√£o pode come√ßar com `C0` ou `C1` pois faria o _code point_ resultante caber no intervalo anterior. No caso de 3 bytes, h√° a possibilidade de o _code point_ equivalente estar no intervalo `D800..DFFF`, e por isso √© separado em 4 intervalos distintos. O primeiro intervalo se preocupa em impedir que ocorra _overlong encoding_, restringindo o segundo byte; o segundo intervalo cont√©m apenas bytes estritamente menores do que `U+D000`; o terceiro intervalo restringe o segundo byte para garantir que seja menor do que `U+D7FF`; o √∫ltimo intervalo representa aqueles estritamente maiores do que `U+DFFF`. Da mesma forma, o caso de 4 bytes √© separado em tr√™s. O primeiro caso se preocupa em impedir _overlong encoding_, enquanto o √∫ltimo caso garante que o _code point_ seja estritamente menor do que `U+10FFFF` (o maior _code point_ v√°lido).

Nessa especifica√ß√£o, n√£o fica clara a rela√ß√£o entre a tabela descritiva e as propriedades intr√≠nsecas ao UTF-8. N√£o √© √≥bvio que h√° uma correspond√™ncia √∫nica entre sequ√™ncias de bytes e _code points_ v√°lidos, nem que todo _code point_ representado pela @UTF8_bytes √© necessariamente v√°lido. Al√©m disso, as opera√ß√µes de extra√ß√£o e concatena√ß√£o de bits, que s√£o oferecidas implicitamente pela @UTF8_bits, n√£o s√£o triviais, e s√£o sucet√≠veis a erros. Com uma especifica√ß√£o complicada demais, √© poss√≠vel que erros sejam cometidos at√© mesmo na concep√ß√£o das regras. Assim, quanto menor o conjunto de regras, mais f√°cil √© de conferir manualmente que elas est√£o corretas.

== Trabalhos relacionados

Faz-se necess√°rio, portanto, estudar como codificadores e decodificadores s√£o especificados e formalizados tradicionalmente. Em geral, para mostrar a *corretude funcional* de ambos, √© interessante mostrar que o codificador e decodificador recuperam os valores de entrada originais um do outro. Isto √©, a grosso modo, mostrar que `encoder a = b` se, e somente, `decoder b = a`.

@Ye2019 descrevem o processo de implementar em Rocq um gerador de codificador e decodificador para Protobuf. Como o protocolo permite que o usu√°rio gere formatos bin√°rios baseado em arquivos de configura√ß√£o, os autores oferecem uma formaliza√ß√£o da sem√¢ntica para os arquivos _protocol buffers_, e utilizam-a para gerar programas que codificam e decodicam os formatos espec√≠ficados em um arquivo, junto das provas de que os programas gerados devem obedecer a essa sem√¢ntica corretamente e que esses necessariamente s√£o inversos um do outro.

@Koprowski2010 forneceram uma implementa√ß√£o similar para linguagens que podem ser descritas por PEGs em Rocq, junto de exemplos pr√°ticos de implementa√ß√µes de parsers de XML e da linguagem Java. @vanGeest2017 desenvolveram uma biblioteca em Agda para descrever pacotes em form√°rios abitr√°rios, focando no caso de uso dos padr√µes ASN.1, fornecendo uma formaliza√ß√£o de formato IPV4. Ambos utilizam a no√ß√£o de inversibilidade entre o codificador e decodificador como fundamento para a corretude.

@thery2004 formalizou uma implementa√ß√£o do algoritmo de Huffman, frequentemente utilizado em padr√µes de compress√£o sem perda de dados. Similarmente @DeflateInCoq2016 construiram uma implementa√ß√£o completa do algoritmo de Deflate, usado em formatos como PNG e GZIP. Para mostrar a corretude, ambos provam a corretude mostrando que o codificador e decodificador s√£o inversos.

@Delaware2019 desenvolveram uma biblioteca em Rocq, _Narcissus_, que permite o usu√°rio de descrever formatos bin√°rios de mensagens em uma DSL dentro do provador interativo. A principal contribui√ß√£o do artigo √© utilizar o maquin√°rio nativo de Rocq para derivar tanto as implementa√ß√µes e as provas utilizando macros de forma que o sistema seja extremamente expressivo. Em casos que a biblioteca n√£o √© forte o suficiente para gerar as provas, o usu√°rio √© capaz de fornecer provas manualmente escritas para a corretude, de forma a estender as capacidades do sistema.

@PulseParse2025 desenvolveram uma biblioteca parecida chamada _PulseParse_ na linguagem F\*, para implementar serializadores e desserializadores para v√°rios formatos: CBOR, um formato bin√°rio inspirado em JSON, e CDDL, uma linguagem que especifica formatos est√°ticos CBOR. Utilizando essa biblioteca, os autores fornecem uma sem√¢ntica ao CDDL e provam a corretude de programas gerados em cima desse conforme essa sem√¢ntica.

Para a simplicidade de implementa√ß√£o, a formaliza√ß√£o dada neste trabalho n√£o utilizar√° nenhuma biblioteca, visto que essas introduzem complexidades especificas de cada DSL. Assim, quase tudo ser√° feito do zero.

#pagebreak()

= Formaliza√ß√£o da especifica√ß√£o

Visto que a especifica√ß√£o fornecida pelo Cons√≥rcio Unicode n√£o √© formal o suficiente, tornou-se necess√°rio estabelecer precisamente quais as propriedades que o codificador e decodificador devem satisfazer pra que sejam considerados corretos. Como visto nos outros trabalhos, √© interessante conseguir provar que quaisquer codificador e decodificador que respeitam a especifica√ß√£o devem necessariamente ser inversos um do outro.

Para garantir a corretude da especifica√ß√£o, √© importante se preocupar com como representar sequ√™ncias de _code points_ e sequ√™ncias de bytes, de forma que seja poss√≠vel aplicar o mapa anterior repetidamente, acumulando seu resultado. Para representar ambos ser√° utilizado o tipo `Z`, que representa o conjunto dos inteiros em Rocq, pois ele possui uma grande gama de propriedades √∫teis j√° provadas previamente, de modo que muitas rela√ß√µes matem√°ticas possam ser reutilizadas. Para representar sequ√™ncias desses, em linguagens funcionais √© tradicional representar strings como listas encadeadas, de forma que tanto as sequ√™ncias de bytes quanto sequ√™ncias de codepoints sejam representados como listas encadeadas de inteiros:

```coq
Definition codepoint : Type := Z.
Definition byte : Type := Z.

Definition unicode_str : Type := list codepoint.
Definition byte_str : Type := list byte.
```

Assim, faz sentido considerar que ambos o codificador e o decodificador sejam fun√ß√µes que mapeiam uma lista de n√∫meros em uma nova lista de n√∫meros, mas isso n√£o leva em considera√ß√£o que ambas podem receber argumentos inv√°lidos. De fato, √© necess√°ria uma maneira de sinalizar que a lista retornada n√£o era uma sequ√™ncia UTF-8 v√°lida.

Para formalizar codificadores e decodificadores, ser√° utilizada a no√ß√£o de _parser_. De modo geral, _parsers_ processam elementos de tipo `A` e retornam algum valor de tipo `B`, e s√£o utilizados quando a transforma√ß√£o pode n√£o funcionar em todos os casos. Assim, √© tradicional utilizar alguma estrutura que envolve o resultado `B` em m√∫ltiplos casos para representar a falibilidade.

O exemplo mais comum dessa estrutura √© `option B`, que pode ser tanto `Some` com um valor de tipo `B`, ou `None`, representando que o `parser` falhou em extrair informa√ß√£o da entrada.
```coq
Inductive option (B :Type) : Type :=
  | Some : B -> option B
  | None : option B.
```

Entretanto, o problema de utilizar o tipo `option` √© que √© poss√≠vel que uma sequ√™ncia de bytes seja quase inteiramente UTF-8 v√°lida, mas tenha algum erro por corrup√ß√£o na hora da transmiss√£o. Nesse caso, o `parser` retornaria `None`, e toda informa√ß√£o seria descartada. Ao inv√©s disso, √© √∫til exigir que o `parser` tente sempre ler o maior n√∫mero de bytes o poss√≠vel do prefixo da entrada, e ao encontrar bytes inv√°lidos, substitua-os pelo caractere '#str.from-unicode(65533)' (`U+FFFD`). Essa pr√°tica √© t√£o difundida que o cap√≠tulo 3.9.6 do padr√£o Unicode d√° guias gerais sobre como essa substitui√ß√£o deve ser feita.

√â poss√≠vel especificar o algoritmo de substitui√ß√£o como um _parser_ que roda o decodificador UTF-8 e substitui as partes inv√°lidas de acordo com o que especificado no cap√≠tulo 3.9.6. Entretanto, este trabalho √© restringido √† leitura do prefixo v√°lido na entrada, e o decodificador que aplica as substitui√ß√µes pode ser feito como extens√£o em um trabalho futuro.

Assim, um _parser_ parcial √© definido como uma fun√ß√£o que recebe uma lista de elementos de tipo `input` e retorna um par de `output` e lista de `input`. A sem√¢ntica de um _parser_ parcial √© de que a lista de `output` representa o resultado de "consumir" o prefixo v√°lido da lista de entrada, enquanto a lista de `input` no resultado representa o sufixo n√£o consumido. Essa sem√¢ntica √© enfor√ßada como propriedades na especifica√ß√£o, vistas mais a frente.

```coq
Definition partial_parser (input output: Type) := list input -> (output * list input).

Definition encoder_type := partial_parser codepoint (list byte).
Definition decoder_type := partial_parser byte (list codepoint).
```

Para especificar unicamente o mapeamento entre sequ√™ncias de bytes e codepoints, devem ser utilizadas as tabelas @UTF8_bits e @UTF8_bytes. Uma poss√≠vel maneira de traduzir isso em c√≥digo Rocq seria com uma propriedade entre uma lista de inteiros e um inteiro, que faz a tradu√ß√£o direta:
```coq
Inductive naive_utf8_map : byte_str -> codepoint -> Prop :=
| OneByte (b1: byte) :
  0x00 <= b1 < 0x80 ->
  naive_utf8_map [b1] b1
| TwoBytes (b1 b2: byte) :
  0xc2 <= b1 <= 0xdf ->
  0x80 <= b2 <= 0xbf ->
  naive_utf8_map [b1; b2] ((b1 mod 64) * 64 + (b2 mod 64))
| ThreeBytes1 (b1 b2 b3: Z):
  b1 = 0xe0 ->
  0xa0 <= b2 <= 0xbf ->
  0x80 <= b3 <= 0xbf ->
  naive_utf8_map [b1; b2; b3] (((b1 - 224) * 4096) + (b2 mod 64) * 64 + (b3 mod 64))
| ThreeBytes2 (b1 b2 b3: Z):
  0xe1 <= b1 <= 0xec \/ 0xee <= b1 <= 0xef ->
  0x80 <= b2 <= 0xbf ->
  0x80 <= b3 <= 0xbf ->
  naive_utf8_map [b1; b2; b3] (((b1 - 224) * 4096) + (b2 mod 64) * 64 + (b3 mod 64))
| ThreeBytes3 (b1 b2 b3: Z):
  b1 = 0xed ->
  0x80 <= b2 <= 0x9f ->
  0x80 <= b3 <= 0xbf ->
  naive_utf8_map [b1; b2; b3] (((b1 - 224) * 4096) + (b2 mod 64) * 64 + (b3 mod 64))
| FourBytes1 (b1 b2 b3 b4: Z):
  b1 = 0xf0 ->
  0x90 <= b2 <= 0xbf ->
  0x80 <= b3 <= 0xbf ->
  0x80 <= b4 <= 0xbf ->
  naive_utf8_map [b1; b2; b3; b4] ((b1 - 240) * 262144 + (b2 mod 64) * 4096 + (b3 mod 64) * 64 + (b4 mod 64))
| FourBytes2 (b1 b2 b3 b4: Z):
  0xf1 <= b1 <= 0xf3 ->
  0x80 <= b2 <= 0xbf ->
  0x80 <= b3 <= 0xbf ->
  0x80 <= b4 <= 0xbf ->
  naive_utf8_map [b1; b2; b3; b4] ((b1 - 240) * 262144 + (b2 mod 64) * 4096 + (b3 mod 64) * 64 + (b4 mod 64))
| FourBytes3 (b1 b2 b3 b4: Z):
  b1 = 0xf4 ->
  0x80 <= b2 <= 0x8f ->
  0x80 <= b3 <= 0xbf ->
  0x80 <= b4 <= 0xbf ->
  naive_utf8_map [b1; b2; b3; b4] ((b1 - 240) * 262144 + (b2 mod 64) * 4096 + (b3 mod 64) * 64 + (b4 mod 64)).
```

Isto √©, um elemento de tipo `naive_utf8_map bytes codepoint` √© uma prova de que a sequ√™ncia de bytes `bytes` mapeia para o codepoint `codepoint` segundo as tabelas @UTF8_bits e @UTF8_bytes. Especificamente, cada construtor de `naive_utf8_map` representa uma das linhas da @UTF8_bytes, e as opera√ß√µes nos bytes de multiplica√ß√£o e `mod` representam como extrair os bits relevantes dos bytes que cont√©m cabe√ßalhos.

O problema de incluir as opera√ß√µes de bits na especifica√ß√£o √© que n√£o h√° como afirmar com certeza que essas opera√ß√µes representam exatamente o que √© dado na @UTF8_bits, visto que as opera√ß√µes de bit shift foram desenvolvidas manualmente. Parte crucial de verifica√ß√£o de software √© que a especifica√ß√£o seja simples de entender, para que seja chec√°vel manualmente por um ser humano, e escrever as opera√ß√µes nos bits diretamente √© um processo que facilmente induz a erros.

Para especificar exatamente qual o mapeamento dado entre bytes e codepoints, √© mais interessante considerar propriedades que esse deve satisfazer. Especificamente, √© simples explicitar as propriedades que ditam o que √© uma sequ√™ncia de bytes UTF-8 v√°lidas (@UTF8_bytes) e o que √© um _code point_ v√°lido, exigindo que o codificador mapeie _code points_ v√°lidos em bytes UTF-8 v√°lidos, e o decodificador mapeie bytes UTF-8 v√°lidos em _code points_ v√°lidos. Entretanto, existem in√∫meras maneiras de fazer esse mapeamento de modo que o codificador e decodificador sejam inversos, e apenas um desses de fato √© o UTF-8.

Assim, uma propriedade muito simples sobre o mapeamento de _code points_ em bytes √© denotada no RFC 3629:
//¬†https://datatracker.ietf.org/doc/html/rfc3629
#quote(block: true, [
    "A ordena√ß√£o lexicogr√°fica por valor dos bytes de strings UTF-8 √© a mesma que se fosse ordenada pelos n√∫meros dos caracteres. √â claro, isso √© de interesse limitado, dado que uma ordena√ß√£o baseada no n√∫mero dos caracteres quase nunca √© culturalmente v√°lida." (@rfc3629)
])

Apesar do que foi dito pelo autor do RFC, essa propriedade √© de extremo interesse para a formaliza√ß√£o por sua simplicidade de enuncia√ß√£o. Para garantir que _code points_ sejam mapeados nas respectivas representa√ß√µes de bytes, basta exigir que tanto o codificador quanto o decodificador respeitem a ordena√ß√£o lexicogr√°fica entre _code points_ e bytes.

Essa propriedade pode ser facilmente compreendida dado dois argumentos informais: os intervalos de  _code points_ representados por sequ√™ncias de bytes de tamanhos diferentes s√£o disjuntos, ou seja, e os cabe√ßalhos no in√≠cio do byte s√£o suficiente para determinar que uma sequ√™ncia de tamanho distinto √© menor ou maior. Isto √©, um _code point_ no primeiro intervalo serializa como `b1 ~ 0xxxxxxx`, e sempre ser√° menor que um no segundo intervalo, que serializa como `b2 ~ 110xxxxx`, o que implica que `b1 < b2`, dado que `b1 <= 0x7F` e `0xC0 <= b2`. O mesmo √© v√°lido para todas as outras compara√ß√µes entre sequ√™ncias de bytes de tamanhos distintos.

Assim, _code points_ de intervalos distintos devem necessariamente serializar para bytes ordenados lexicograficamente, visto que o primeiro byte √© suficiente para determinar qual √© maior. Basta agora mostrar que _code points_ de um mesmo intervalo devem serializar ordenadamente. Mas isso √© trivial, visto todos os bytes das duas sequ√™ncias devem ter cabe√ßalhos id√™nticos, e os bits do _code point_ s√£o serializados ordenadamente, fazendo com que comparar lexicograficamente as sequ√™ncias de bytes seja equivalente a comparar os bits dos _code points_ originais. 

Ou seja, √© poss√≠vel caracterizar unicamente codificadores e decodificadores de UTF-8 como fun√ß√µes que mapeiam bytes no formato da @UTF8_bytes em _code points_ v√°lidos (e vice versa), *que respeitam a ordena√ß√£o entre _code points_ e bytes*. Essa √© a principal ideia por tr√°s da especifica√ß√£o deste trabalho, e o objetivo da formaliza√ß√£o √© mostrar que apenas essas propriedades s√£o suficientes para provar que quaisquer par de codificador e decodificador que respeitam-a devem ser inversos.

Uma vantagem pr√°tica de utilizar a compara√ß√£o para identificar o mapeamento na especifica√ß√£o, ao inv√©s das opera√ß√µes em bits, √© que n√£o √© necess√°rio mostrar que o _code point_ tem o mesmo valor exato. √â poss√≠vel que uma implementa√ß√£o utilize opera√ß√µes distintas e chegue no mesmo resultado correto. Por exemplo, √© razo√°vel imaginar que um usu√°rio deseje implementar opera√ß√µes espec√≠ficas de bit _shifts_ e _masks_ que n√£o utilizam multiplica√ß√£o e `mod`, e torna-se parte da prova mostrar que as opera√ß√µes devem ser iguais numericamente. Utilizar a compara√ß√£o oferece mais flexibilidade ao usu√°rio que prova a especifica√ß√£o, pois tudo que √© necess√°rio √© dizer que a opera√ß√£o escolhida √© crescente.

Tendo isso em mente, definimos as seguintes nota√ß√µes:
```coq
Definition codepoint : Type := Z.
Definition byte : Type := Z.

Definition unicode_str : Type := list codepoint.
Definition byte_str : Type := list byte.
Definition codepoints_compare := List.list_compare Z.compare.
Definition bytes_compare := List.list_compare Z.compare.
```

As fun√ß√µes `codepoints_compare` e `bytes_compare` s√£o utilizadas exatamente para prover as compara√ß√µes entre inteiros. A fun√ß√£o `Z.compare` √© oferecida pela biblioteca padr√£o do Rocq, recebendo dois inteiros e retorna o resultado da compara√ß√£o entre eles, do tipo `comparison`:
```coq
Inductive comparison : Set :=
  | Eq : comparison
  | Lt : comparison
  | Gt : comparison.
```

A fun√ß√£o `list_compare` transforma uma compara√ß√£o entre elementos de um tipo `T` em uma compara√ß√£o entre elementos de tipo `list T`, utilizando a sem√¢ntica de compara√ß√£o lexicogr√°fica. In√∫meras propriedades sobre as fun√ß√µes de compara√ß√£o `Z.compare` e `list_compare` j√° s√£o oferecidas por padr√£o, como antissimetria, transitividade e reflexividade. De fato, veremos mais a frente que essas propriedades s√£o chave para provar o teorema sobre a unicidade do mapeamento.

Em seguida, definimos as propriedades necess√°rias para afirmar que um `codepoint` arbitr√°rio, isto √©, um inteiro qualquer, √© um _code point_ UTF-8 v√°lido. Como visto anteriormente, basta saber que esse est√° entre `0x0` e `0x10FFFF`, e n√£o est√° no intervalo `0xD800..0xDFFF` . Isso pode ser representado como as seguintes tr√™s propriedades:
```coq
Definition codepoint_less_than_10ffff (code: codepoint) : Prop :=
  (code <= 0x10ffff).

Definition codepoint_is_not_surrogate (code: codepoint) : Prop :=
  (code < 0xd800) \/ (code > 0xdfff).

Definition codepoint_not_negative (code: codepoint): Prop :=
  (code >= 0).

Definition valid_codepoint (code: codepoint) := codepoint_less_than_10ffff code /\ codepoint_is_not_surrogate code /\ codepoint_not_negative code.
```

Isto √©, provar que `valid_codepoint code` para algum `code` significa mostrar que as tr√™s propriedades valem ao mesmo tempo.

Para definir o tipo `valid_codepoint_representation`, ser√° utilizada a mesma ideia do `naive_utf8_map`. Isto √©, esse s√≥ pode ser construido quando os elementos da lista de entrada est√£o nos intervalos de alguma das linhas da tabela, e representa afirmar que uma certa lista de bytes √© a representa√ß√£o em UTF-8 de *algum* _code point_. Diferentemente do `naive_utf8_map`, n√£o √© dado o valor do _code point_ espec√≠fico que essa sequ√™ncia mapeia, e apenas se afirma que essa √© v√°lida segundo @UTF8_bytes.

```coq
Inductive valid_codepoint_representation : list Z -> Prop :=
| OneByte (b: Z) :
  0 <= b <= 0x7f ->
  valid_codepoint_representation [b]
| TwoByte (b1 b2: Z):
  0xc2 <= b1 <= 0xdf ->
  0x80 <= b2 <= 0xbf ->
  valid_codepoint_representation [b1; b2]
| ThreeByte1 (b1 b2 b3: Z):
  b1 = 0xe0 ->
  0xa0 <= b2 <= 0xbf ->
  0x80 <= b3 <= 0xbf ->
  valid_codepoint_representation [b1; b2; b3]
| ThreeByte2 (b1 b2 b3: Z):
  0xe1 <= b1 <= 0xec \/ 0xee <= b1 <= 0xef ->
  0x80 <= b2 <= 0xbf ->
  0x80 <= b3 <= 0xbf ->
  valid_codepoint_representation [b1; b2; b3]
| ThreeByte3 (b1 b2 b3: Z):
  b1 = 0xed ->
  0x80 <= b2 <= 0x9f ->
  0x80 <= b3 <= 0xbf ->
  valid_codepoint_representation [b1; b2; b3]
| FourBytes1 (b1 b2 b3 b4: Z):
  b1 = 0xf0 ->
  0x90 <= b2 <= 0xbf ->
  0x80 <= b3 <= 0xbf ->
  0x80 <= b4 <= 0xbf ->
  valid_codepoint_representation [b1; b2; b3; b4]
| FourBytes2 (b1 b2 b3 b4: Z):
  0xf1 <= b1 <= 0xf3 ->
  0x80 <= b2 <= 0xbf ->
  0x80 <= b3 <= 0xbf ->
  0x80 <= b4 <= 0xbf ->
  valid_codepoint_representation [b1; b2; b3; b4]
| FourBytes3 (b1 b2 b3 b4: Z):
  b1 = 0xf4 ->
  0x80 <= b2 <= 0x8f ->
  0x80 <= b3 <= 0xbf ->
  0x80 <= b4 <= 0xbf ->
  valid_codepoint_representation [b1; b2; b3; b4].
```

Com isso, existem duas maneiras de construir uma lista de bytes v√°lidos UTF-8: ou a lista √© vazia, ou ela √© a concatena√ß√£o de uma representa√ß√£o em bytes de um `codepoint` e uma lista de bytes UTF-8 v√°lidos. O tipo que representa que essa rela√ß√£o √©:
```coq
Inductive valid_utf8_bytes: list Z ->  Prop :=
| Utf8Nil : valid_utf8_bytes []
| Utf8Concat (bytes tail: list Z) :
    valid_codepoint_representation bytes ->
    valid_utf8_bytes tail ->
    valid_utf8_bytes (bytes ++ tail).
```

Apenas essas defini√ß√µes s√£o suficientes para come√ßar a definir as propriedades que o codificador e decodificador devem seguir:
```coq
Definition encoder_nil (encoder: encoder_type) := encoder [] = ([], []).
```
A primeira propriedade dita que o `encoder` deve aceitar a lista vazia com o resultado vazio.

```coq
Definition encoder_input_correct_iff (encoder: encoder_type) := forall code,
    valid_codepoint code <->
    exists bytes, encoder [code] = (bytes, []).
```

A segunda propriedade √© uma dupla implica√ß√£o: da esquerda para direita, diz que o `encoder` deve aceitar todo `codepoint` v√°lido; da direita para esquerda, diz que se o encoder aceita uma lista com um `codepoint` apenas, ent√£o esse `codepoint` √© v√°lido.

```coq
Definition encoder_output_correct (encoder: encoder_type) := forall code,
    match encoder [code] with
    | (bytes, []) => valid_codepoint_representation bytes
    | (bytes, rest) => bytes = [] /\ rest = [code]
    end.
```
A terceira propriedade descorre sobre a validade do resultado de um `encoder`. Apenas dois resultados ao chamar um `encoder` com uma lista de um elemento s√£o poss√≠veis: ou a entrada √© aceita, e os `bytes` √† esquerda s√£o uma representa√ß√£o de codepoints v√°lida, ou n√£o √© aceita, o que implica que os `bytes` devem ser vazios, e o lado n√£o consumido deve conter o `codepoint` da entrada. 

```coq
Definition encoder_strictly_increasing (encoder: encoder_type) := forall code1 code2 bytes1 bytes2,
    encoder [code1] = (bytes1, nil) ->
    encoder [code2] = (bytes2, nil) ->
    Z.compare code1 code2 = bytes_compare bytes1 bytes2.
```

A quarta propriedade afirma que o `encoder` respeita a ordena√ß√£o lexicogr√°fica entre bytes e _code points_, explicada anteriormente. Essa propriedade √© suficiente para afirmar que o `encoder` mapeia o _code point_ na sua respectiva representa√ß√£o em bytes, de acordo com o mapeamento UTF-8.

```coq
Definition encoder_projects (encoder: encoder_type) := forall xs ys,
    encoder (xs ++ ys) =
      match encoder xs with
      | (bytes, nil) =>
          let (bytes2, rest) := encoder ys in
          (bytes ++ bytes2, rest)
      | (bytes, rest) => (bytes, rest ++ ys)
      end.
```

Por fim, a quinta e √∫ltima propriedade √© a que descreve como o `encoder` deve se comportar perante listas grandes. Quando uma lista pode ser quebrada em duas listas menores, o resultado de chamar o `encoder`  na lista maior √© igual a chamar na primeira, e se for aceita, chamar na segunda e concatenar os resultados. No caso de erro, o encoder para imediatamente.

```coq
Record utf8_encoder_spec encoder := {
    enc_nil : encoder_nil encoder;
    enc_increasing : encoder_strictly_increasing encoder;
    enc_input : encoder_input_correct_iff encoder;
    enc_output : encoder_output_correct encoder;
    enc_projects : encoder_projects encoder;
  }.
```

Apenas essas 5 propriedades s√£o o suficiente para qualificar um `encoder` como um codificador de UTF-8 v√°lido, segundo a especifica√ß√£o. Importantemente, n√£o √© necess√°rio ter um decodificador para provar que o codificador est√° correto. Para provar que um `encoder` est√° certo, basta construir um elemento de tipo `utf8_encoder_spec encoder`.

As propriedades que o decodificador deve satisfazer s√£o an√°logas √†s do codificador.
```coq
Definition decoder_nil (decoder: decoder_type) := decoder nil = (nil, nil).

Definition decoder_output_correct (decoder: decoder_type) := forall bytes suffix codes,
    decoder bytes = (codes, suffix) ->
    valid_codepoints codes /\
      (exists prefix,
          decoder prefix = (codes, [])
          /\ valid_utf8_bytes prefix
          /\ bytes = prefix ++ suffix).

Definition decoder_input_correct_iff (decoder: decoder_type) := forall bytes,
    valid_codepoint_representation bytes <->
    exists code, decoder bytes = ([code], []).

Definition decoder_strictly_increasing (decoder: decoder_type) := forall bytes1 bytes2 code1 code2,
    decoder bytes1 = ([code1], nil) ->
    decoder bytes2 = ([code2], nil) ->
    Z.compare code1 code2 = bytes_compare bytes1 bytes2.

Definition decoder_projects (decoder: decoder_type) := forall xs ys,
    valid_codepoint_representation xs ->
    decoder (xs ++ ys) =
      let (codes, _) := decoder xs in
      let (codes2, rest) := decoder ys in
      (codes ++ codes2, rest).

Record utf8_decoder_spec decoder := {
    dec_nil : decoder_nil decoder;
    dec_input : decoder_input_correct_iff decoder;
    dec_output : decoder_output_correct decoder;
    dec_increasing : decoder_strictly_increasing decoder;
    dec_projects : decoder_projects decoder;
  }.
```
A primeira propriedade afirma que todo `decoder` aceita a lista vazia. A segunda afirma que do _code point_ emitido pelo `decoder` deve ser v√°lido. A terceira fala que todo input v√°lido deve ser aceito. A quarta propriedade afirma sobre a ordena√ß√£o entre bytes e _code points_, assim como no `decoder`. A quinta propriedade √© uma propriedade de comuta√ß√£o para desconstruir listas em listas menores.

Com essas duas defini√ß√µes, a especifica√ß√£o UTF-8 completa para um par codificador e decodificador √© o par que cont√©m a especifica√ß√£o para o codificador e decodificador separadamente. Por serem disjuntos, √© poss√≠vel mostrar que quaisquer `encoder` e `decoder` s√£o corretos mostrando que as regras valem para eles separadamente.
```coq
Record utf8_spec encoder decoder := {
    encoder_spec_compliant : utf8_encoder_spec encoder;
    decoder_spec_compliant : utf8_decoder_spec decoder;
  }.
```

Para ter certeza de que a especifica√ß√£o est√° correta, √© necess√°rio provar teoremas sobre ela. Como visto anteriormente, a propriedades principal que formar√° o cerne da corretude da especifica√ß√£o √© de que todo par `(encoder, decoder)` que implemente `utf8_spec encoder decoder` deve necessariamente ser inverso um do outro.

Por ambos o codificador e decodificador serem _parser_ parciais, √© preciso considerar que nem toda entrada ir√° ser aceita, e isso √© levado em conta da seguinte forma: toda entrada deve necessariamente ter um prefixo UTF-8 v√°lido -- que pode ser a lista vazia --  de forma que o prefixo v√°lido deve ser a entrada para o processador dual.

```coq
Theorem utf8_spec_encoder_decoder_inverse : forall encoder decoder,
    utf8_spec encoder decoder ->
    forall codes bytes codes_suffix,
      encoder codes = (bytes, codes_suffix) ->
      exists codes_prefix, decoder bytes = (codes_prefix, nil) /\ codes = (codes_prefix ++ codes_suffix)%list.

Theorem utf8_spec_decoder_encoder_inverse_strong : forall encoder decoder,
    utf8_spec encoder decoder ->
    forall codes bytes bytes_suffix,
      decoder bytes = (codes, bytes_suffix) ->
      exists bytes_prefix, encoder codes = (bytes_prefix, nil) /\ bytes = (bytes_prefix ++ bytes_suffix)%list.
Proof.
```
Isto √©, se `encoder codes = (bytes, codes_suffix)`, ent√£o necessariamente deve existir um prefixo `codes_prefix` tal que `decoder bytes = (codes_prefix, [])` e `codes = codes_prefix ++ codes_suffix`.

Para provar essas propriedades, muito trabalho √© necess√°rio. Intuitivamente, a prova √© inteiramente baseada no fato de que ordena√ß√£o implica em existir apenas uma fun√ß√£o que respeite o mapeamento entre bytes e _code points_, entretanto isso n√£o √© nem um pouco √≥bvio. Assim, √© necess√°rio mostrar esse fato para que possa ser utilizado nas provas seguintes.

== Ordena√ß√µes em conjuntos finitos

Para utilizar a ordena√ß√£o produtivamente na prova, precisamos mostrar que exigir a ordena√ß√£o √© equivalente a completamente especificar a fun√ß√£o de mapeamento. Para isso, √© interessante considerar o conjunto de inteiros que esses mapeiam, pois provas sobre inteiros s√£o mais f√°ceis de entender e manipular.

Tanto `valid_codepoint` quanto `valid_codepoint_representation` s√£o propriedades que formam conjuntos finitos de exato mesmo tamanho (`0x10FFFF - 0x800` elementos, o n√∫mero de _code points_ v√°lidos). Por serem conjuntos finitos, √© poss√≠vel assinalar um inteiro para cada elemento, seu *√≠ndice*. Provar que a ordena√ß√£o implica em apenas uma fun√ß√£o significa provar que *existe apenas um mapeamento ordenado entre conjuntos finitos de mesmo tamanho*.

#quote(block: true, [Defini√ß√£o: o *√≠ndice* de um _code point_ √© o n√∫mero que representa a posi√ß√£o desse na ordena√ß√£o. ])

Como o conjunto de _code points_ v√°lidos possui uma descontinuidade no intervalo `U+D800..U+DFFF`, esse √≠ndice pode ser entendido como o pr√≥prio valor do _code point_ quando √© menor que `U+D800`, e valor do _code point_ subtraido de `0x800` quando maior, de forma que $"index"("U+D7FF") = 1 + "index"("U+E000")$. Assim, fica claro que o conjunto de √≠ndices √© exatamente o intervalo de inteiros entre `0 <= n < (0x10FFFF - 0x800)`.

#quote(block: true, [Defini√ß√£o: o *√≠ndice* de uma sequ√™ncia de bytes √© o √≠ndice do _code point_ que essa representa. ])

Com essas defini√ß√µes, fica claro que √© poss√≠vel mapear cada _code point_ e cada sequ√™ncia de bytes em um inteiro unicamente, atrav√©s das fun√ß√µes de `nth_valid_codepoint` -- que retorna o en√©simo _code point_ v√°lido dado seu √≠ndice -- e `nth_valid_codepoint_representation` -- que retorna a sequ√™ncia de bytes do en√©simo _code point_ v√°lido. Al√©m disso, o mapeamento em √≠ndices apenas pula descontinuidades, ent√£o esse deve ser ordenado.

Como queremos que os codificadores e decodificadores sejam inversos, √© natural considerar que as fun√ß√µes de √≠ndices s√£o invers√≠veis. De fato, ambas formam bije√ß√µes com o conjunto dos √≠ndices, e preservam a ordena√ß√£o entre elementos, e portanto podem ser consideradas isomorfismos ordenados entre o cojunto de _code points_/sequ√™ncias de bytes e o conjunto dos √≠ndices. No mais, os codificadores e decodificadores, segundo a especifica√ß√£o, tamb√©m s√£o formam uma bije√ß√£o ordenada, dessa vez diretamente entre _code points_ v√°lidos e sequ√™ncias de bytes.

Assim, a prova crucial para utilizar a ordena√ß√£o consiste em mostrar que quaisquer dois codificadores e decodificadores que seguem a especifica√ß√£o corretamente devem se comportar como se consultassem `nth_valid_codepoint` e `nth_valid_codepoint_representation` internamente. Informalmente, √© equivalente a provar que todo codificador deve transformar o en√©simo _code point_ na en√©sima sequ√™ncia de bytes.

Para formalizar essa no√ß√£o, √© necess√°rio definir o que s√£o isomorfismos parciais ordenados. Primeiro, s√£o definidos morfismos parciais:
```coq
Definition interval (count n : Z) : Prop :=
  (0 <= n /\ n < count)%Z.

Record PartialMorphism {X Y}
  (domain : X -> Prop) (range : Y -> Prop) (f : X -> option Y) : Prop :=  {
    always_in_range : forall x y, domain x -> f x = Some y -> range y;
    not_domain_none: forall x, f x = None -> (not (domain x))
  }.

Definition and_then {X Y Z}
  (f : X -> option Y) (g : Y -> option Z) : X -> option Z :=
  fun x =>
    match (f x) with
    | Some y => (g y)
    | None => None
    end.

Definition pointwise_equal {X Y}
  (domain : X -> Prop) (f g : X -> option Y) : Prop :=
  forall x, domain x -> f x = g x.
```

Isto √©, um morfismo parcial √© uma fun√ß√£o `f: X -> option Y` que possui duas propriedades, `domain` e `range`, de forma que sempre que `x` est√° no dom√≠nio -- `domain x` -- e `f x = Some y` para algum `y`, ent√£o `y` est√° na imagem -- `range y`. Note que n√£o √© necess√°rio provar que `f x` sempre √© `Some`, pois √© poss√≠vel de provar isso utilizando `not_domain_none`:

```coq
Theorem partial_morphism_elimination {X Y}
  {domain : X -> Prop} {range : Y -> Prop} {f : X -> option Y} :
  PartialMorphism domain range f ->
  forall (x : X),
    domain x ->
  exists y,
    ((range y) /\ (f x = Some y)).
Proof.
  intros [f_some f_none] x domain_x.
  destruct (f x) as [y|] eqn:f_x.
  - exists y. repeat split. apply (f_some x y); assumption.
  - apply f_none in f_x. apply f_x in domain_x. exfalso. auto.
Qed.
```

Vale ressaltar que s√≥ √© exigido que `y` esteja na imagem quando `x` est√° no dom√≠nio, o que significa que `f` pode retornar `Some` para elementos fora do dom√≠nio. Isso √© feito para suportar certas defini√ß√µes naturais, como que `fun x => Some x` √© a fun√ß√£o identidade, bem como provar que essa √© um automorfismo ordenado de todo conjunto. Isso tamb√©m significa que n√£o √© poss√≠vel provar `domain x` dado `f x = Some y`.

A defini√ß√£o `pointwise_equal f g` √© utilizada no lugar da igualdade `f = g`, pois provar igualdade de fun√ß√µes em Coq a partir da igualdade de elementos n√£o √© poss√≠vel; isto √©, n√£o √© poss√≠vel provar que `f = g` com a hip√≥tese de que `pointwise_equal f g` sem adicionar axiomas externos (extensionalidade funcional).

Com isso, definimos o que √© um isomorfismo parcial:
```coq
Record PartialIsomorphism {T1 T2}
  (domain: T1 -> Prop) (range: T2 -> Prop)
  (to: T1 -> option T2) (from: T2 -> option T1) := {
    from_morphism : PartialMorphism domain range to;
    to_morphism: PartialMorphism range domain from;
    from_to_id : pointwise_equal domain (and_then to from) (fun x => Some x);
    to_from_id : pointwise_equal range (and_then from to) (fun x => Some x);
  }.
```

Um isomorfismo parcial √© um par de morfismos `from` e `to` que mapeiam entre conjuntos `T1` e `T2`, junto de provas de que a composi√ß√£o deles d√° a identidade. 

Para encapsular a ordena√ß√£o, definimos a no√ß√£o de conjunto ordenado, que √© um tipo `T` munido de uma opera√ß√£o de compara√ß√£o `compare` que respeita as seguintes propriedades:
```coq
Record Ordered {T} (compare: T -> T -> comparison) := {
    eq : forall t1 t2, compare t1 t2 = Eq <-> t1 = t2;
    antisym : forall t1 t2, compare t1 t2 = CompOpp (compare t2 t1);
    trans : forall t1 t2 t3 res,
              compare t1 t2 = res ->
              compare t2 t3 = res ->
              compare t1 t3 = res;
  }.
```
Ou seja, para prova provar que um tipo `T` √© ordenado, basta mostrar que existe uma rela√ß√£o de compara√ß√£o em `T` reflexiva, antisim√©trica e transitiva. Al√©m disso, a no√ß√£o de uma fun√ß√£o `to` ser "crescente" √© caracterizada da seguinte forma:
```coq
Definition increasing {T1 T2}
  (domain: T1 -> Prop) (compare1: T1 -> T1 -> comparison)
  (compare2: T2 -> T2 -> comparison) (to: T1 -> option T2) :=
  forall n n' m m',
    (domain n) -> (domain m) ->
    to n = Some n' ->
    to m = Some m' ->
    (compare1 n m) = (compare2 n' m').
```
Informalmente, uma fun√ß√£o `f` √© `increasing` se `compare1 n m = compare2 (f n) (f m)`, ou seja, se respeita a compara√ß√£o entre quaisquer dois elementos. √â necess√°rio exigir que ambos `n` e `m` estejam no dom√≠nio, pois consideramos `f` como um morfismo parcial, que pode retornar `Some` para elementos de fora do dom√≠nio.

Por fim, definimos um isomorfismo parcial ordenado como um par de conjuntos ordenados `T1` e `T2` que possuem um isomorfismo parcial, e que ao menos um dos mapeamentos √© `increasing`, por simplicidade, o `to`.
```coq
Record OrderedPartialIsomorphism {T1 T2}
  (domain: T1 -> Prop) (range: T2 -> Prop)
  (compare1: T1 -> T1 -> comparison) (compare2: T2 -> T2 -> comparison)
  (to: T1 -> option T2) (from: T2 -> option T1)
   := {
    opi_ordered1 : @Ordered T1 compare1;
    opi_ordered2 : @Ordered T2 compare2;
    opi_isomorphism : @PartialIsomorphism T1 T2 domain range to from;
    opi_to_preserves_compare : increasing domain compare1 compare2 to;
  }.
```

Apenas precisamos provar que um deles √© `increasing`, visto que √© relativamente simples provar que o `from` √© `increasing` em seu respectivo dom√≠nio. De fato, provamos esse teorema no futuro, e ele √© parte necess√°ria para provar o teorema principal.

Para entender o argumento principal da ordena√ß√£o de isomorfismos, √© √∫til considerar a seguinte estrutura de isomorfismos entre o conjunto dos inteiros menores que `count`.

#image("isomorphism.png")

Dados dois conjuntos ordenados `T1` e `T2`, bem como isomorfismos desses para `interval count`, n√£o √© dif√≠cil mostrar que compor `from0` com `to2` com `to1` forma um automorfismo do conjunto de √≠ndices. Al√©m disso, como todas as fun√ß√µes intermedi√°rias respeitam a ordena√ß√£o de seus respectivos conjuntos, esse automorfismo deve ser ordenado. Mas o √∫nico morfismo parcial que sai do conjunto dos inteiros e chega no conjunto dos inteiros respeitando a ordena√ß√£o √© a fun√ß√£o identidade! De fato, queremos mostrar que

#align(center, $lambda x. "Some" x = "and_then" "to0" ("and_then" "to2" "from1")$)

Para isso, precisamos provar 3 teoremas: que todo morfismo parcial ordenado √© `pointwise_equal` com a identidade, que `and_then to0 (and_then to2 from1)` √© um automorfismo de `interval count`, e que esse respeita a ordena√ß√£o.

O primeiro passo √© representado no seguinte teorema:

```coq
Theorem interval_ordered_automorphism_is_id :
  forall (n: Z),
  (0 <= n)%Z ->
  forall (to : Z -> option Z),
    PartialMorphism (interval n) (interval n) to ->
    increasing (interval n) Z.compare Z.compare to ->
    pointwise_equal (interval n) to (fun x => Some x).
```

A prova desse teorema √© feita resolvida utilizando indu√ß√£o em `n`. O caso base √© trivialmente resolvido, visto que temos na hip√≥tese um elemento `x` tal que `0 <= x < 0`.

O passo indutivo se baseia em mostrar que necessariamente `to n = Some n`. Para isso, sabemos que n deve pelo menos algum elemento na imagem `interval (Z.succ n)`, uma vez que `n < Z.succ n` trivialmente, logo `to n = Some n'`. Tamb√©m sabemos que `n'` √© menor que `Z.succ n`, ent√£o podemos quebrar a prova em dois casos, `n' = n`, exatamente queremos mostrar, ou `n' < n`, onde precisamos derivar alguma incoer√™ncia.

Para mostrar que `n' < n` √© falso, precisamos mostrar duas propriedades que nos permitem reduzir a imagem e o dom√≠nio de um automorfismo. A primeira delas nos diz quando podemos reduzir a imagem:
```coq
Lemma tighten_ordered_morphism (n m m' : Z) (to : Z -> option Z) :
  PartialMorphism (interval (Z.succ n)) (interval m) to ->
  increasing (interval (Z.succ n)) Z.compare Z.compare to ->
  interval m m' ->
  to n = Some m' ->
  PartialMorphism (interval (Z.succ n)) (interval (Z.succ m')) to.
```
Este teorema nos permite limitar a imagem do morfismo quando a `interval (Z.succ m')` quando sabemos que `to n = Some m'` e `m' < m`. Intuitivamente, isso √© v√°lido porque tudo que prometemos sobre o resultado √© que ele faz parte da imagem, mas √© poss√≠vel escolher um `m` grande demais, de forma que o maior elemento do dom√≠nio n√£o chegue at√© o maior elemento da imagem.

A segunda propriedade nos diz quando podemos restringir o dom√≠nio e a imagem ao mesmo tempo:
```coq
Lemma ordered_morphism_restriction (n m n' m' : Z) (to : Z -> option Z) :
  PartialMorphism (interval n) (interval m) to ->
  increasing (interval n) Z.compare Z.compare to ->
  interval n n' ->
  interval m m' ->
  to n' = Some m' ->
  PartialMorphism (interval n') (interval m') to.
```

Da mesma forma, podemos restringir `to` a um morfismo entre `interval n'` e `interval m'` se mostrarmos que o maior elemento do dom√≠nio, `n'`, mapeia em `m'`, que `n'` est√° contido em `interval n` e que `m'` est√° contido em `interval m`.

Para derivar a contradi√ß√£o, basta mostrar que n√£o podemos restringir demais a imagem:
```coq
Theorem no_ordered_morphism_to_smaller_interval : forall (n m : Z) (to : Z -> option Z),
  (0 <= m)%Z ->
  (m < n)%Z ->
  PartialMorphism (interval n) (interval m) to ->
  increasing (interval n) Z.compare Z.compare to ->
  False.
```

Esse teorema afirma que n√£o podemos restringir um automorfismo ordenado para uma imagem menor que o tamanho do dom√≠nio, pois algum elemento do domin√≠o necessariamente deve ser ignorado nesse caso. Esse √© provado por indu√ß√£o, utilizando os dois lemmas acima, mostrando que aplicar as restri√ß√µes v√°lidas sempre levam em contradi√ß√µes.

Com esse teorema, podemos mostrar que `n' < n` implica em podermos limitarmos a imagem do morfismo (por `tighten_ordered_morphism`), gerando um morfismo cuja imagem √© menor que o dom√≠nio, o que √© uma contradi√ß√£o.

Tendo provado `interval_ordered_automorphism_is_id`, podemos provar o seguinte teorema:
```coq
Theorem finite_partial_isomorphism_unique {T0 T1} (count: Z) (range0: T0 -> Prop) (range1: T1 -> Prop) compare0 compare1:
  (0 <= count)%Z ->
  forall from0 from1 to0 to1 to2,
  OrderedPartialIsomorphism (interval count) range0 Z.compare compare0 to0 from0 ->
  OrderedPartialIsomorphism (interval count) range1 Z.compare compare1 to1 from1 ->
  PartialMorphism range0 range1 to2 ->
  increasing range0 compare0 compare1 to2 ->
  pointwise_equal range0 to2 (and_then from0 to1).
```

Como citado anteriormente, a prova desse √© feita em 3 fases. Primeiro, mostramos que a composi√ß√£o de fun√ß√µes supracitada √© um automorfismo de `interval count`:
```coq
Proof.
... 
assert (PartialMorphism (interval count) (interval count)
  (and_then to0 (and_then to2 from1))) as morphism.
```
Depois, mostramos que esse morfismo √© crescente no mesmo intervalo:
```coq
...
assert (increasing (interval count) Z.compare Z.compare
              (and_then to0 (and_then to2 from1))) as increasing_composition.
```
E por fim, mostramos que podemos desfazer todas as opera√ß√µes com suas inversas chegando em `pointwise_equal to2 (and_then from0 to1)`, como quer√≠amos demonstrar.

Como esse teorema em m√£os, basta construir isomorfismos tais que `to0 = nth_valid_codepoint` e `to1 = nth_valid_codepoint_representation`, com `from0/from1` sendo suas respectivas inversas. Utilizando-o, conseguimos mostrar que qualquer outro morfismo `to2` -- em particular, um codificador que respeita a especifica√ß√£o -- deve ser `pointwise_equal` √† composi√ß√£o dessas duas fun√ß√µes, mostrando sua unicidade. Trivialmente, a unicidade do decodificador tamb√©m √© resolvida apenas trocando a ordem dos isomorfismos.

== √çndices de codepoints e de sequ√™ncias de bytes

Assim, precisamos construir os isomorfismos supracitados. √© necess√°rio lembrar que o conjunto de √≠ndices exclui codepoints no intervalo `0xD800..0xDFFF`, ou seja, o √≠ndice deve "pular" esse intervalo. Assim, a √∫nica preocupa√ß√£o da fun√ß√£o `nth_valid_codepoint` √© somar `0x800` quando isso acontece:

```coq
Definition nth_valid_codepoint (n: Z) : option codepoint :=
  if n <? 0 then
    None
  else if n <? 0xd800 then
    Some n
  else if n <=? 0x10ffff - 0x0800 then
    Some (n + 0x0800)
  else
    None.
```

Para mostrar que essa fun√ß√£o forma um isomorfismo parcial, as seguintes propriedades s√£o provadas:
```coq
Lemma nth_valid_codepoint_is_some_iff_valid : forall code,
    (exists n, nth_valid_codepoint n = Some code) <->
      valid_codepoint code.

Lemma nth_valid_codepoint_none : forall n,
    nth_valid_codepoint n = None ->
    n < 0 \/ n > (0x10ffff - 0x800).

Lemma nth_valid_codepoint_increasing : forall n1 code1 n2 code2,
    nth_valid_codepoint n1 = Some code1 ->
    nth_valid_codepoint n2 = Some code2 ->
    Z.compare n1 n2 = Z.compare code1 code2.
```

A prova desses teoremas √© omitida por brevidade, mas todas envolvem observar as compara√ß√µes feitas em `nth_valid_codepoint` e utilizar a t√°tica `lia` para casos espec√≠ficos, que resolve rela√ß√µes na aritm√©tica de Presburgo. Em especial, a prova de que respeita a compara√ß√£o √© feita considerando todas as poss√≠veis maneiras que os `if`s podem se desdobrar, e mostrar que em todas elas as compara√ß√µes s√£o iguais.

Al√©m disso, √© necess√°rio oferecer a fun√ß√£o inversa dessa, que vai do √≠ndice do codepoint para o codepoint:
```coq
Definition inverse_nth_valid_codepoint (code: codepoint) : option Z :=
  if (code <? 0) then
    None 
  else if (code <? 0xd800) then
    Some code
  else if (code <=? 0x10ffff)%Z then
    Some (code - 0x0800)%Z
  else
    None.
```

Bem como provar que ambas s√£o inversas:
```coq
Lemma nth_valid_codepoint_invertible : forall code n,
    nth_valid_codepoint n = Some code <->
      inverse_nth_valid_codepoint code = Some n /\ valid_codepoint code.
```

Assim, √© poss√≠vel provar que essa fun√ß√£o forma um isomorfismo parcial ordenado, construindo um elemento do seguinte tipo:
```coq
Definition codepoint_nth_isomorphism : OrderedPartialIsomorphism (interval (0x10ffff - 0x7ff)) valid_codepoint Z.compare codepoint_compare nth_valid_codepoint inverse_nth_valid_codepoint.
```
Recapitulando, `codepoint_nth_isomorphism` √© a prova de que o par (`nth_valid_codepoint`, `inverse_nth_valid_codepoint`) formam um isomorfimo com o conjunto de √≠ndices, e esse isomorfismo respeita a ordena√ß√£o de codepoints e a ordena√ß√£o de √≠ndices. A constru√ß√£o dessa prova utiliza todos os lemmas supracitados, bem como a prova de que o conjunto dos inteiros √© um conjunto ordenado:
```coq
Definition ZOrder : @Ordered Z Z.compare.
  split. apply Z.compare_eq_iff. intros. apply Z.compare_antisym.
  intros. destruct res.
  - apply Z.compare_eq_iff in H, H0. subst. apply Z.compare_refl.
  - apply Zcompare.Zcompare_Lt_trans with (m := t2); assumption.
  - apply Zcompare.Zcompare_Gt_trans with (m := t2); assumption.
Qed.
```

Ap√≥s isso, √© necess√°rio definir o mesmo para `nth_valid_code_representation`.
```coq
Definition nth_valid_codepoint_representation (n: Z) : option byte_str :=
  let n := if Z.ltb n 0xd800 then n else n + 0x800 in
  if (n <? 0) then
    None
  else if (n <=? 127) then
    Some [ n ]
  else if (n <=? 0x7ff) then
    let b1 := n / 64 in
    let b2 := n mod 64 in
    Some [ 192 + b1; 128 + b2]
  else if (n <=? 0xffff) then
    let r := n / 64 in
    let b1 := r / 64 in
    let b2 := r mod 64 in
    let b3 := n mod 64 in
    Some [ 224 + b1; 128 + b2; 128 + b3]
  else if (n <=? 0x10ffff) then
    let r1 := n / 64 in
    let r2 := r1 / 64 in
    let b1 := r2 / 64 in
    let b2 := r2 mod 64 in
    let b3 := r1 mod 64 in
    let b4 := n mod 64 in
    Some [ 240 + b1; 128 + b2; 128 + b3; 128 + b4]
  else
    None.
```

E provar os mesmos lemmas:
```coq
Lemma nth_valid_codepoint_representation_spec: forall bytes,
    (exists n, nth_valid_codepoint_representation n = Some bytes) <->
      valid_codepoint_representation bytes.

Lemma nth_valid_codepoint_representation_none : forall n : Z,
    nth_valid_codepoint_representation n = None -> 
    n < 0 \/ n > (1114111 - 2048).

Lemma nth_valid_codepoint_representation_compare_compat: forall n1 n2 bytes1 bytes2,
    nth_valid_codepoint_representation n1 = Some bytes1 -> 
    nth_valid_codepoint_representation n2 = Some bytes2 -> 
    Z.compare n1 n2 = bytes_compare bytes1 bytes2.
```

A prova desses √© mais complexa, pois a fun√ß√£o que mapeia o √≠ndice na sequ√™ncia de bytes equivalente √© muito mais complexa. Para facilitar a an√°lise, t√°ticas especiais foram criadas para automatizar a resolu√ß√£o de casos parecidos utilizando a t√°tica `lia`.

Tamb√©m √© necess√°rio desenvolver a fun√ß√£o que calcula o √≠ndice do codepoint a partir da sequ√™ncia de bytes.

```coq
Definition inverse_nth_valid_codepoint_representation (bytes: byte_str) : option Z :=
  let between b lo hi := andb (lo <=? b) (b <=? hi) in 
  match bytes with
  | [b] => if between b 0 127 then Some b else None
  | [b1; b2] =>
      if andb (between b1 0xc2 0xdf) (between b2 0x80 0xbf) then
        Some ((b1 mod 64) * 64 + (b2 mod 64))
      else None
  | [b1; b2; b3] =>
      let fst := andb (andb (b1 =? 0xe0) (between b2 0xa0 0xbf)) (between b3 0x80 0xbf) in
      let snd := andb (andb (between b1 0xe1 0xec) (between b2 0x80 0xbf)) (between b3 0x80 0xbf) in
      let trd := andb (andb (b1 =? 0xed) (between b2 0x80 0x9f)) (between b3 0x80 0xbf) in
      let frth := andb (andb (between b1 0xee 0xef) (between b2 0x80 0xbf)) (between b3 0x80 0xbf) in
      let n := ((b1 - 224) * 64 * 64) + (b2 mod 64) * 64 + (b3 mod 64) in
      if orb (orb fst snd) trd then
        Some n
      else if frth then
        Some (n - 2048)
      else 
        None
  | [b1; b2; b3; b4] =>
      let fst := andb (andb (andb (b1 =? 0xf0) (between b2 0x90 0xbf)) (between b3 0x80 0xbf)) (between b4 0x80 0xbf) in
      let snd := andb (andb (andb (between b1 0xf1 0xf3) (between b2 0x80 0xbf)) (between b3 0x80 0xbf)) (between b4 0x80 0xbf) in
      let trd := andb (andb (andb (b1 =? 0xf4) (between b2 0x80 0x8f)) (between b3 0x80 0xbf)) (between b4 0x80 0xbf) in
      if orb (orb fst snd) trd then
        Some ((b1 - 240) * 64 * 64 * 64 + (b2 mod 64) * 64 * 64 + (b3 mod 64) * 64 + (b4 mod 64) - 0x800)
      else None
  | _ => None
  end.
```

Vale notar que as opera√ß√µes que essa executa s√£o exatamente as mesmas opera√ß√µes dadas em `naive_utf8_map`, mas dessa vez, a corretude dessas opera√ß√µes √© checada no fato de que essa √© a inversa da `nth_valid_codepoint_representation`:

```coq
Lemma nth_valid_codepoint_representation_invertible : forall n bytes,
    nth_valid_codepoint_representation n = Some bytes ->
      inverse_nth_valid_codepoint_representation bytes = Some n.

Lemma inverse_nth_valid_codepoint_representation_invertible : forall bytes n,
    valid_codepoint_representation bytes ->
    inverse_nth_valid_codepoint_representation bytes = Some n ->
    nth_valid_codepoint_representation n = Some bytes.
```

Por fim, tamb√©m √© necess√°rio provar que o conjunto de sequ√™ncias de bytes √© ordenado, de acordo com a compara√ß√£o lexicogr√°fica.

```coq
Definition BytesOrder : Ordered bytes_compare.
Proof.
  unfold bytes_compare.
  split.
  - apply list_compare_refl. apply Z.compare_eq_iff.
  - intros.
    apply list_compare_antisym. apply Z.compare_eq_iff. apply Z.compare_antisym.
  - intros.
    apply list_compare_trans with (ys:=t2); try assumption.
    + apply Z.compare_eq_iff.
    + intros. destruct c.
      -- apply Z.compare_eq_iff in H1, H2. subst. apply Z.compare_refl.
      -- apply Zcompare.Zcompare_Lt_trans with (m := y); assumption.
      -- apply Zcompare.Zcompare_Gt_trans with (m := y); assumption.
    + apply Z.compare_antisym.
Qed.
```

Assim, √© poss√≠vel provar que o par (`nth_valid_codepoint_representation`, `inverse_nth_valid_codepoint_representation`) forma um isomorfismo com o conjunto dos inteiros de `0x10ffff - 0x7ff`, e que esse isomorfismo respeita a ordena√ß√£o:
```coq
Theorem valid_codepoint_representation_isomorphism :
    OrderedPartialIsomorphism (interval (0x10ffff - 0x7ff)) valid_codepoint_representation Z.compare bytes_compare nth_valid_codepoint_representation inverse_nth_valid_codepoint_representation.
```

== Corretude da especifica√ß√£o

Desde o in√≠cio, o objetivo de mostrar essas propriedades de ordena√ß√£o e de √≠ndice √© utilizar `finite_partial_isomorphism_unique` para provar os seguintes teoremas:
```coq
Lemma utf8_spec_implies_encoder_maps_nth_to_nth : forall encoder,
    utf8_encoder_spec encoder ->
    forall code bytes,
      encoder [code] = (bytes, []) -> 
      exists n, nth_valid_codepoint n = Some code /\ nth_valid_codepoint_representation n = Some bytes.

Lemma utf8_spec_implies_decoder_maps_nth_to_nth : forall decoder,
    utf8_decoder_spec decoder ->
    forall code bytes,
      decoder bytes = ([code], []) -> 
      exists n, nth_valid_codepoint n = Some code /\ nth_valid_codepoint_representation n = Some bytes.
```

Isto √©, quando um codificador aceita um codepoint, ent√£o o resultado √© a sequ√™ncia de bytes com o √≠ndice equivalente. Da mesma forma, quando o decodificador aceita uma sequ√™ncia de bytes, ent√£o o resultado √© o codepoint com o √≠ndice equivalente. 

Para utilizar o teorema de ordena√ß√£o nessa prova, √© necess√°rio construir morfismos parciais (que retornam `option` ao inv√©s de listas) a partir de codificadores e decodificadores:
```coq
Definition encoder_to_option (encoder: encoder_type) code :=
  match encoder [code] with
  | (bytes, []) => Some bytes
  | _ => None
  end.

Definition decoder_to_option (decoder: decoder_type) bytes :=
  match decoder bytes with
  | ([code], []) => Some code
  | _ => None
  end.
```

Assim, os seguintes lemmas sobre `encoder` e `decoder` s√£o provados, para que possam ser utilizados nas provas:
```coq
Lemma encoder_partial_morphism : forall encoder,
    utf8_encoder_spec encoder -> 
    partial_morphism valid_codepoint valid_codepoint_representation (encoder_to_option encoder).

Lemma decoder_partial_morphism : forall decoder,
    utf8_decoder_spec decoder ->
    partial_morphism valid_codepoint_representation valid_codepoint (decoder_to_option decoder).

Lemma encoder_to_option_increasing : forall encoder,
    utf8_encoder_spec encoder ->
    increasing valid_codepoint Z.compare bytes_compare (encoder_to_option encoder).

Lemma decoder_to_option_increasing: forall decoder,
    utf8_decoder_spec decoder ->
    increasing valid_codepoint_representation bytes_compare Z.compare (decoder_to_option decoder).
```

Todos esses lemmas apenas estendem as propriedades dos codificadores e decodificadores para o morfismo parcial. Com os lemmas de mapeamento de `n` pra `n` em m√£os, √© trivial mostrar que tanto o `encoder` quanto o `decoder` devem ser inversos no caso de apenas um codepoint:

```coq
Theorem utf8_spec_encoder_decoder_inverse_single: forall encoder decoder,
    utf8_encoder_spec encoder ->
    utf8_decoder_spec decoder ->
    forall code bytes,
      encoder [code] = (bytes, []) ->
      decoder bytes = ([code], []).

Theorem utf8_spec_decoder_encoder_inverse_single: forall encoder decoder,
    utf8_encoder_spec encoder ->
    utf8_decoder_spec decoder ->
    forall code bytes,
      decoder bytes = ([code], []) ->
      encoder [code] = (bytes, []).
```

Provar esses teoremas se reduz a aplicar o teorema de mapeamento `n` em `n`, e mostrar que podemos transformar √≠ndices em _code points_ / bytes utilizando as inversas. Esses teoremas s√£o suficientes para provar o teorema da corretude da especifica√ß√£o do codificador:
```coq
Theorem utf8_spec_encoder_decoder_inverse : forall encoder decoder,
    utf8_encoder_spec encoder ->
    utf8_decoder_spec decoder ->
    forall codes bytes codes_suffix,
      encoder codes = (bytes, codes_suffix) ->
      exists codes_prefix, decoder bytes = (codes_prefix, nil) /\ codes = (codes_prefix ++ codes_suffix)%list.
```

Esse √© trivialmente provado por indu√ß√£o na lista de entrada, aplicando `utf8_spec_encoder_decoder_inverse_single` no _code point_ extra√≠do.

Para provar a corretude do decodificador, mais trabalho √© necess√°rio, visto que indu√ß√£o na lista de entrada n√£o √© uma estrat√©gia suficiente. Ao inv√©s isso, gostar√≠amos de fazer a indu√ß√£o na lista de _code points_ da sa√≠da, visto que essa √© muito mais simples de entender.

Com esse objetivo, provamos que o decodificador tem uma propriedade dual √† proje√ß√£o do codificador:

```coq
Theorem utf8_spec_decoder_project_dual : forall encoder decoder,
    utf8_encoder_spec encoder ->
    utf8_decoder_spec decoder ->
    forall xs ys bytes bytes_suffix,
      decoder bytes = (xs ++ ys, bytes_suffix) ->
      (exists bytes1 bytes2,
          decoder bytes1 = (xs, []) /\ 
            decoder bytes2 = (ys, []) /\
            bytes = bytes1 ++ bytes2 ++ bytes_suffix).
```

Com essa propriedade, podemos provar por indu√ß√£o que ambos s√£o inversos:

```coq
Theorem utf8_spec_decoder_encoder_inverse: forall encoder decoder,
    utf8_encoder_spec encoder ->
    utf8_decoder_spec decoder ->
    forall codes bytes bytes_suffix,
      decoder bytes = (codes, bytes_suffix) ->
      exists bytes_prefix, encoder codes = (bytes_prefix, nil) /\ bytes = (bytes_prefix ++ bytes_suffix)%list.
```

Utilizamos a propriedade dual da proje√ß√£o para separar `decoder bytes = (code :: codes, suffix)` em `decoder bytes1 = ([code], [])`, onde podemos aplicar o teorema de que s√£o inversos para um elemento.

#pagebreak()

= Implementa√ß√£o

Com a especifica√ß√£o feita, a implementa√ß√£o de um codificador e decodificador pr√°ticos √© relativamente simples. Para implementar o codificador, primeiro √© definida uma fun√ß√£o que mapeia um codepoint numa sequ√™ncia de bytes:
```coq
Definition utf8_encode_codepoint (n: codepoint) : @option (list byte) :=
  if (n <? 0) then
    None
  else if (n <=? 127) then
    Some [ n ]
  else if (n <=? 0x7ff) then
    let b1 := n / 64 in
    let b2 := n mod 64 in
    Some [ 192 + b1; 128 + b2]
  else if (andb (n <=? 0xffff) (orb (n <? 0xd800) (n >? 0xdfff))) then
    let r := n / 64 in
    let b1 := r / 64 in
    let b2 := r mod 64 in
    let b3 := n mod 64 in
    Some [ 224 + b1; 128 + b2; 128 + b3]
  else if (andb (n <=? 0x10ffff) (n >? 0xffff)) then
    let r1 := n / 64 in
    let r2 := r1 / 64 in
    let b1 := r2 / 64 in
    let b2 := r2 mod 64 in
    let b3 := r1 mod 64 in
    let b4 := n mod 64 in
    Some [ 240 + b1; 128 + b2; 128 + b3; 128 + b4]
  else
    None.
```
Assim, o codificador √© definido como uma fun√ß√£o que recursivamente mapeia o mapeamento acima, parando quando a lista acaba ou quando o mapeamento retorna `None`.
```coq
Fixpoint utf8_encode (unicode: unicode_str) : (list byte) * (list codepoint) :=
  match unicode with
  | [] => ([], [])
  | code :: unicode_rest =>
      match utf8_encode_codepoint code with
      | None => ([], code :: unicode_rest)
      | Some bytes => 
          let (bytes_rest, unicode_rest) := utf8_encode unicode_rest in
          (bytes ++ bytes_rest, unicode_rest)
      end
  end.
```

// https://bjoern.hoehrmann.de/utf-8/decoder/dfa/
Para implementar o decodificador, √© utilizado um aut√¥mato de estado finito. Um DFA pode ser derivado observando cada linha da @UTF8_bytes, e considerando quais caracteres podem ser lidos em cada parte.

#image("dfa-bytes.png")

A partir desse grafo, define-se o conjunto de poss√≠veis estados, e uma enumera√ß√£o de todos os poss√≠veis estados √∫teis que aparecem no grafo:
```coq
Inductive parsing_state :=
  Initial
| Expecting_1_80_BF
| Expecting_2_80_BF
| Expecting_3_80_BF
| Expecting_2_80_9F
| Expecting_2_A0_BF
| Expecting_3_90_BF
| Expecting_3_80_8F.

Inductive byte_range :=
| Range_00_7F 
| Range_80_8F
| Range_90_9F
| Range_A0_BF
| Range_C2_DF
| Byte_E0      
| Range_E1_EC
| Byte_ED
| Range_EE_EF
| Byte_F0
| Range_F1_F3
| Byte_F4
.

Definition byte_range_dec (b: byte) : option byte_range :=
  if b <? 0 then
    None
  else if b <=? 0x7f then
    Some Range_00_7F
  else if b <=? 0x8f then
    Some Range_80_8F
  else if b <=? 0x9f then
    Some Range_90_9F
  else if b <=? 0xbf then
    Some Range_A0_BF
  else if b <=? 0xc1 then
    None
  else if b <=? 0xdf then
    Some Range_C2_DF
  else if b  =? 0xe0 then
    Some Byte_E0
  else if b <=? 0xec then
    Some Range_E1_EC
  else if b  =? 0xed then
    Some Byte_ED
  else if b <=? 0xef then
    Some Range_EE_EF
  else if b  =? 0xf0 then
    Some Byte_F0
  else if b <=? 0xf3 then
    Some Range_F1_F3
  else if b  =? 0xf4 then
    Some Byte_F4
  else
    None.
```

Tamb√©m s√£o definidas fun√ß√µes auxiliares para representar as opera√ß√µes de extra√ß√£o de bits relevantes.
```coq
Definition push_bottom_bits (carry: codepoint) (b: byte): codepoint :=
  carry * 64 + (b mod 64).

Definition extract_7_bits (b: byte) : codepoint :=
  b mod 128.

Definition extract_5_bits (b: byte) : codepoint :=
  b mod 32.

Definition extract_4_bits (b: byte) : codepoint :=
  b mod 16.

Definition extract_3_bits (b: byte) : codepoint :=
  b mod 8.
```
Por fim, √© definida a fun√ß√£o `next_state`, que calcula o pr√≥ximo estado do DFA a partir do estado atual e do byte visto. Para representar o fim de um codepoint, √© criado o tipo `parsing_result`:
```coq
Inductive parsing_result :=
  Finished (codep: codepoint)
| More (state: parsing_state) (acc: codepoint).

Definition next_state (state: parsing_state) (carry: codepoint) (b: byte) : @option parsing_result :=
  match (state, byte_range_dec b) with
  | (Initial, Some Range_00_7F) => Some (Finished (extract_7_bits b))
  | (Initial, Some Range_C2_DF) => Some (More Expecting_1_80_BF (extract_5_bits b))
  | (Initial, Some Byte_E0)     => Some (More Expecting_2_A0_BF (extract_4_bits b))
  | (Initial, Some Range_E1_EC)
  | (Initial, Some Range_EE_EF) => Some (More Expecting_2_80_BF (extract_4_bits b))
  | (Initial, Some Byte_ED)     => Some (More Expecting_2_80_9F (extract_4_bits b))
  | (Initial, Some Byte_F0)     => Some (More Expecting_3_90_BF (extract_3_bits b))
  | (Initial, Some Range_F1_F3) => Some (More Expecting_3_80_BF (extract_3_bits b))
  | (Initial, Some Byte_F4)     => Some (More Expecting_3_80_8F (extract_3_bits b))
  | (Initial, _) => None
  | (Expecting_1_80_BF, Some Range_A0_BF)
  | (Expecting_1_80_BF, Some Range_90_9F)
  | (Expecting_1_80_BF, Some Range_80_8F) => Some (Finished (push_bottom_bits carry b))
  | (Expecting_2_80_BF, Some Range_80_8F)
  | (Expecting_2_80_BF, Some Range_90_9F)
  | (Expecting_2_80_9F, Some Range_80_8F)
  | (Expecting_2_80_9F, Some Range_90_9F)
  | (Expecting_2_80_BF, Some Range_A0_BF) => Some (More Expecting_1_80_BF (push_bottom_bits carry b))
  | (Expecting_3_80_BF, Some Range_80_8F)
  | (Expecting_3_80_BF, Some Range_90_9F)
  | (Expecting_3_80_BF, Some Range_A0_BF)
  | (Expecting_3_90_BF, Some Range_90_9F)
  | (Expecting_3_90_BF, Some Range_A0_BF)
  | (Expecting_3_80_8F, Some Range_80_8F) => Some (More Expecting_2_80_BF (push_bottom_bits carry b))
  | (Expecting_2_A0_BF, Some Range_A0_BF) => Some (More Expecting_1_80_BF (push_bottom_bits carry b))
  | (Expecting_3_80_8F, Some Range_90_9F)
  | (Expecting_3_80_8F, Some Range_A0_BF) => None
  | _ => None
  end.
```

A fun√ß√£o do decodificador, ent√£o, √© uma fun√ß√£o que recursivamente calcula o pr√≥ximo estado utilizando `next_state`. Quando o resultado √© um codepoint finalizado, a fun√ß√£o volta para o estado inicial e come√ßa a ler um novo codepoint.
```coq
Fixpoint utf8_dfa_decode_rec (bytes: list byte) (carry: codepoint) (state: parsing_state) (consumed: list byte)
  : unicode_str * (list byte) :=
  match bytes with
  | nil => ([], consumed)
  | cons b rest =>
      match next_state state carry b with
      | Some (Finished codep) =>
          let (vals, rest) := utf8_dfa_decode_rec rest 0x00 Initial [] in
          (codep :: vals, rest)
      | Some (More state codep) =>
          utf8_dfa_decode_rec rest codep state (consumed ++ [b])
      | None => ([], consumed ++ bytes)
      end
  end.

Definition utf8_dfa_decode (bytes: list byte) : unicode_str * (list byte) :=
  utf8_dfa_decode_rec bytes 0x00 Initial [].
```

Note que, pelas restri√ß√µes de ser um _parser_ parcial, √© necess√°rio guardar os bytes consumidos equivalentes ao codepoint atual, de modo a n√£o jogar fora bytes se apenas um da sequ√™ncia for inv√°lido. Isso √© necess√°rio para provar que essa fun√ß√£o siga a especifica√ß√£o dada anteriromente.

Como refor√ßado anteriormente, a corretude da implementa√ß√£o est√° inteiramente baseada em provar que ambos codificador e decodificador seguem a especifica√ß√£o desenvolvida. Dado todo o desenvolvimento, fica extremamente claro o significado de "provar que segue a especifica√ß√£o": construir um elemento cujo tipo √© `utf8_spec utf8_encode utf8_dfa_decode`.

Para fazer isso, basta construir dois elementos, um de tipo `utf8_encode_spec utf8_encode`, e outro de tipo `utf8_decode_spec utf8_dfa_decode`. Como visto anteriormente, isso significa provar os cinco lemmas para `utf8_encode` e cinco lemmas para `utf8_decode`.

== Provando a corretude do codificador

A prova de que `utf8_encode [] = ([], [])` se reduz a computar o lado esquerdo e provar a igualdade:
```coq
Lemma utf8_encode_nil : encoder_nil utf8_encode.
Proof.
  reflexivity.
Qed.
```
Para provar `encoder_input_correct_iff`, √© √∫til mostrar primeiro que a fun√ß√£o que transforma um codepoint em bytes (`utf8_encode_codepoint`) est√° correta:

```coq
Lemma utf8_encode_codepoint_input : forall code,
    valid_codepoint code <->
    exists bytes, utf8_encode_codepoint code = Some bytes.
Proof.
  intro code; split. 
  - intro valid_code.
    destruct (utf8_encode_codepoint code) as [bytes |] eqn:encode_code.
    + exists bytes. reflexivity.
    + unfold utf8_encode_codepoint in encode_code.
      destruct valid_code as [c1 [c2 c3]].
      unfold codepoint_less_than_10ffff in c1.
      unfold codepoint_is_not_surrogate in c2.
      unfold codepoint_not_negative in c3.
      crush_comparisons; try discriminate; lia.
  - intros [bytes encode_code].
    unfold utf8_encode_codepoint in encode_code.
    unfold valid_codepoint, codepoint_less_than_10ffff, codepoint_is_not_surrogate, codepoint_not_negative.
    crush_comparisons; try discriminate; lia.
Qed.
```
Vale ressaltar que essa prova mostra uma das for√ßas principais do Coq: t√°ticas de automa√ß√£o customizadas. A t√°tica `crush_comparisons` fora criada especificamente para reescrever hip√≥teses que cont√©m `if _ then _ else` e destru√≠-las em dois _goals_, um onde se prova o caso em que a condi√ß√£o √© verdadeira, e outro onde a condi√ß√£o √© falsa. 
```coq
Ltac crush_comparisons :=
  repeat match goal with
    | [G: context[if (?a <=? ?b)%N then _ else _] |- _] => 
        let l := fresh "less_than_eq" in
        destruct (a <=? b)%N eqn:l; [apply Z.leb_le in l| apply Z.leb_nle in l]
    | [G: context[if (?a <? ?b)%N then _ else _] |- _] => 
        let l := fresh "less_than" in
        destruct (a <? b)%N eqn:l; [apply Z.ltb_lt in l| apply Z.ltb_nlt in l]
    | [G: context[if (?a >? ?b)%N then _ else _] |- _] => 
        rewrite Z.gtb_ltb in G
    | [G: context[if (andb ?a ?b) then _ else _] |- _] =>
        rewrite Bool.andb_if in G
    | [G: context[if (orb ?a ?b) then _ else _] |- _] =>
        rewrite Bool.orb_lazy_alt in G
    end.
```

Assim, n√£o √© necess√°rio manualmente provar cada um dos casos utilizando as provas matem√°ticas espec√≠ficas, o que √© muito mais trabalhoso. Com esse lemma, a prova de que todo codepoint unit√°rio √© levado em uma sequ√™ncia de bytes, e toda sequ√™ncia de bytes tem um codepoint equivalente, √© simples:
```coq
Lemma utf8_encode_correct : encoder_input_correct_iff utf8_encode.
Proof.
  intros code. split.
  - intro valid_code.
    destruct (utf8_encode [code]) as [bytes rest] eqn: enc.
    exists bytes. apply pair_equal_spec. repeat split.
    simpl in enc.
    apply utf8_encode_codepoint_input in valid_code.
    destruct valid_code as [bytes2 enc_code]. rewrite enc_code in enc.
    inversion enc. reflexivity.
  - intros [bytes enc_code].
    simpl in enc_code.
    destruct (utf8_encode_codepoint code) as [bytes2 |] eqn:enc_single; [| discriminate].
    inversion enc_code. subst.
    apply utf8_encode_codepoint_input.
    exists bytes2. assumption.
Qed.
```

A prova de `utf8_encode_output`, que afirma que toda sequ√™ncia de bytes deve ser `valid_codepoint_representation`, tamb√©m √© similarmente simples: basta descontruir a fun√ß√£o em todos os poss√≠veis casos em que um codepoint pode ser mapeado, e depois provar que todos eles est√£o certos utilizando `lia`. Para isso, outra t√°tica customizada √© utilizada, `add_bounds`, que adiciona provas sobre desigualdades envolvendo `mod` ao contexto, para que a t√°tica `lia` possa provar teoremas envolvendo compara√ß√µes.
```coq
Lemma utf8_encode_output : encoder_output_correct utf8_encode.
Proof.
  intros code.
  destruct (utf8_encode [code]) as [bytes rest] eqn:encode_single.
  simpl in encode_single.
  destruct (utf8_encode_codepoint code) as [bytes2 |] eqn:encode_code; [| inversion encode_single; split; reflexivity].
  assert (exists bytes, utf8_encode_codepoint code = Some bytes) as code_valid. exists bytes2. assumption.
  apply utf8_encode_codepoint_input in code_valid.
  unfold valid_codepoint, codepoint_less_than_10ffff, codepoint_is_not_surrogate, codepoint_not_negative in code_valid.
  destruct code_valid as [c1 [c2 c3]].
  inversion encode_single. rewrite app_nil_r in *. subst.
  unfold utf8_encode_codepoint in encode_code.
  crush_comparisons; try discriminate; try lia; rewrite <- some_injective in encode_code; subst.
  + apply OneByte. lia.
  + add_bounds (code mod 64). apply TwoByte; lia.
  + add_bounds (code mod 64).
    add_bounds ((code / 64) mod 64).
    destruct c2.
    * destruct (code / 64 / 64 =? 0) eqn:is_e0.
      -- apply ThreeByte1; lia.
      -- destruct (code <? 0xd000) eqn:code_less_d000.
         --- apply ThreeByte2. left. all: lia.
         --- apply ThreeByte3; lia.
    * apply ThreeByte2. right. all: lia.
  + add_bounds (code mod 64). add_bounds (code / 64 mod 64). apply ThreeByte2; try lia.
  + add_bounds (code mod 64).
    add_bounds (code / 64 mod 64).
    add_bounds ((code / 64 / 64) mod 64).
    destruct (code / 64 / 64 / 64 =? 0) eqn:is_f0.
    * apply FourBytes1; try lia.
    * destruct (code / 64 / 64 / 64 =? 4) eqn:is_f4.
      -- apply FourBytes3; try lia.
      -- apply FourBytes2; try lia.
Qed.
```

√â interessante notar que os 5 _goals_ resultantes est√£o diretamente relacionados com as 5 maneiras que um `codepoint` pode ser considerado correto: uma maneira para cada intervalo de 1, 2 e 4 bytes, e 2 maneiras no intervalo de 3 bytes -- pode tanto ser menor que `0xDB00` quanto maior que `0xDFFF`.

A prova de que o codificador pode ser projetado corretamente sobre listas menores √© trivial, e se resume a afirmar que concatena√ß√£o de listas √© comutativa:
```coq
Lemma utf8_encode_projects : encoder_projects utf8_encode.
Proof.
  intro xs. induction xs as [|x xs]; intros ys.
  - rewrite utf8_encode_nil. rewrite app_nil_l.
    destruct (utf8_encode ys). reflexivity.
  - rewrite <- app_comm_cons.
    unfold utf8_encode. fold utf8_encode.
    destruct (utf8_encode_codepoint x) as [bytes |]eqn:encode_x.
    + rewrite IHxs.
      destruct (utf8_encode xs). destruct (utf8_encode ys).
      destruct l0. rewrite app_assoc. reflexivity. reflexivity.
    + rewrite app_comm_cons. reflexivity.
Qed.
```

Por fim, o teorema de que `utf8_encode` √© crescente √© facilmente resolvido utilizando a combina√ß√£o de `crush_comparisons` e `lia`. 

```coq
Lemma utf8_encode_increasing: encoder_strictly_increasing utf8_encode.
Proof.
  intros code1 code2 bytes1 bytes2 encode_code1 encode_code2.
  simpl in encode_code1, encode_code2.
  destruct (utf8_encode_codepoint code1) as [bytes1'|] eqn:enc_code1; [|inversion encode_code1].
  destruct (utf8_encode_codepoint code2) as [bytes2'|] eqn:enc_code2; [|inversion encode_code2]. rewrite app_nil_r in *.
  inversion encode_code1. inversion encode_code2. subst.
  clear encode_code1. clear encode_code2.
  unfold utf8_encode_codepoint in enc_code1, enc_code2.
  crush_comparisons; try discriminate; try lia; rewrite <- some_injective in enc_code1; rewrite <- some_injective in enc_code2; subst; unfold bytes_compare, list_compare.
  1: destruct (code1 ?= code2); reflexivity.
  all: (repeat match goal with
          | |- context[match ?a ?= ?b with | _ => _ end] =>
              let comp := fresh "compare" in
              add_bounds a; add_bounds b;
              destruct (Z.compare_spec a b) as [comp | comp | comp]
          end);
    match goal with
    | [|- (?n1 ?= ?n2 = Eq)] => apply Z.compare_eq_iff
    | [|- (?n1 ?= ?n2 = Lt)] => fold (Z.lt n1 n2)
    | [|- (?n1 ?= ?n2 = Gt)] => fold (Z.gt n1 n2)
    end; subst; try discriminate; lia.
Qed.
```
Na prova deste teorema h√° duas hip√≥teses contendo `utf8_encode` distintos no contexto, o que significa que `crush_comparisons` desconstr√≥i em 289 casos distintos, a maioria deles com hip√≥teses inv√°lidas, como `None = Some _`, ou `code1 < coe2` e `code2 < code1`. A sequ√™ncia `try discriminate; try lia` resolvem essas imediatamente. Como resultado, sobram exatamente 25 = $5 * 5$ goals, o produto cartesiano de todas as poss√≠veis maneiras que dois codepoints podem ser v√°lidos, e todos esses involvem compara√ß√µes entre elementos de mesmo tamanho, e s√£o facilmente resolvidos por `lia`.

Por fim, √© enunciada a prova de que essa fun√ß√£o de fato segue a especifica√ß√£o dada anteriormente:

```coq
Theorem utf8_encode_spec_compliant : utf8_encoder_spec utf8_encode.
Proof.
  split.
  - apply utf8_encode_nil.
  - apply utf8_encode_increasing.
  - apply utf8_encode_correct.
  - apply utf8_encode_output.
  - apply utf8_encode_projects.
Qed.
```

== Provando a corretude do decodificador

Assim como no caso do codificador, provar que `utf8_dfa_decode [] = ([], [])` √© trivialmente resolvido por `reflexivity`.

```coq
Lemma utf8_dfa_nil : decoder_nil utf8_dfa_decode.
Proof.
  reflexivity.
Qed.
```

Para provar que `utf8_dfa_decode` projeta sobre entradas v√°lidas pode ser provado utilizando uma t√°tica auxiliar `lia_simplify`, que tenta simplificar compara√ß√µes quando `lia` consegue provar que essas devem ser verdadeiras ou falsas. Duas vers√µes s√£o dadas, `lia_simplify` que atua diretamente no _goal_, e `lia_simplify_hyp`, que atua em uma hip√≥tese.
```coq
Ltac lia_simplify :=
  repeat match goal with
    | |- context[match (if ?cond then ?a else ?b) with | _ => _ end] =>
        ((replace cond with false by lia) || (replace cond with true by lia) || (destruct cond))
    end.

Ltac lia_simplify_hyp H :=
  repeat match type of H with
    | context[match (if ?cond then ?a else ?b) with | _ => _ end] =>
        (replace cond with false in H by lia)
            || (replace cond with true in H by lia)
            || let C := fresh "cond" in destruct cond eqn:C
    end.

Lemma utf8_dfa_projects : decoder_projects utf8_dfa_decode.
Proof.
  intros xs ys valid_xs.
  unfold utf8_dfa_decode.
  destruct valid_xs; simpl; unfold next_state, byte_range_dec; lia_simplify; 
    destruct (utf8_dfa_decode_rec ys 0 Initial []); reflexivity.
Qed.
```

Para os outros 3 teoremas, dois lemmas centrais sobre `utf8_dfa_decode` ser√£o utilizados. O primeiro afirma que quando a o prefixo UTF-8 v√°lido √© `[]`, ent√£o a parte inv√°lida deve ser igual √† entrada dada a fun√ß√£o:
```coq
Lemma utf8_dfa_decode_invalid: forall bytes suffix,
    utf8_dfa_decode bytes = ([], suffix) ->
    bytes = suffix.
Proof.
  intros bytes suffix decode_bytes.
  unfold utf8_dfa_decode in decode_bytes.
  destruct bytes as [| byte1 bytes].
  - simpl in decode_bytes. inversion decode_bytes. reflexivity.
  - repeat lazymatch goal with
           | [NextState: context[next_state ?state ?carry ?byte] |- _] =>
               unfold next_state in NextState;
               let range := fresh "range" in
               destruct (byte_range_dec byte) as [range|];
               [| inversion NextState; reflexivity];
               destruct range;
               try (inversion NextState; reflexivity)
           | [Decode: context[utf8_dfa_decode_rec (?byte :: ?rest) ?code ?state ?consumed] |- _] =>
               simpl in Decode
           | [Decode: context[utf8_dfa_decode_rec ?bytes 0 Initial ?consumed] |- _] =>
               destruct (utf8_dfa_decode_rec bytes 0 Initial); inversion Decode
           | [Decode: context[utf8_dfa_decode_rec ?bytes ?code ?state ?consumed] |- _] =>
               let byte := fresh "byte" in
               let rest := fresh "bytes" in
               destruct bytes as [| byte rest]; simpl in Decode; [inversion Decode; reflexivity|]
           end.
Qed.
```

Novamente, a estrat√©gia dessa prova se resume em destruir todas as poss√≠veis maneiras que uma sequ√™ncia de bytes pode ser rejeitada, e mostrar que em todas elas `bytes = suffix`.

O segundo teorema afirma que, quando o resultado cont√©m ao menos um _code point_ `code`, ent√£o esse deve ser v√°lido, e deve haver um prefixo `prefix` UTF-8 v√°lido tal que `utf8_decode prefix = ([code], [])`.
```coq
Lemma utf8_dfa_decode_prefix: forall bytes code codes suffix,
    utf8_dfa_decode bytes = (code :: codes, suffix) ->
    exists prefix rest,
      valid_codepoint code /\
        valid_codepoint_representation prefix /\ 
        utf8_dfa_decode prefix = ([code], []) /\
        utf8_dfa_decode rest = (codes, suffix) /\
        bytes = prefix ++ rest.
```

A prova desse lemma √© significativamente mais complicada, dado que o objetivo √© provar uma conjun√ß√£o de 5 proposi√ß√µes. Ela pode ser entendida em duas fases: primeiro, todos as poss√≠veis maneiras de que um `byte` pode ser considerado v√°lido s√£o separadas em diferentes _goals_; depois, as proposi√ß√µes s√£o provadas utilizando t√°ticas espec√≠ficas, uma para cada afirma√ß√£o da conjun√ß√£o.

A combina√ß√£o de `utf8_dfa_decode_invalid` e `utf8_dfa_decode_prefix` √© tudo que √© preciso para provar provas sobre `utf8_dfa_decode` utilizando indu√ß√£o. Como bytes que representam codepoints podem ter de 1 a 4 elementos de tamanho, provas de indu√ß√£o na lista de entrada s√£o fracas demais para serem √∫teis, e √© muito mais natural fazer a indu√ß√£o na lista de sa√≠da de _code points_. Assim, esses dois lemmas cont√©m todas as propriedades cruciais que ser√£o necess√°rias para provar os pr√≥ximos teoremas.

A prova de que toda lista de sa√≠da de `utf8_dfa_decode` √© `valid_utf8` √© resolvida com uma simples indu√ß√£o na lista de _code points_ do resultado:
```coq
Lemma utf8_dfa_output : decoder_output_correct utf8_dfa_decode.
Proof.
  intros bytes suffix codes decode_bytes.
  generalize dependent bytes.
  induction codes as [| code codes].
  - split. constructor.
    exists []. repeat split. constructor.
    apply utf8_dfa_decode_invalid in decode_bytes.
    subst. reflexivity.
  - intros bytes decode_bytes.
    apply utf8_dfa_decode_prefix in decode_bytes as G.
    destruct G as [prefix [rest [valid_code [valid_prefix [decode_prefix [decode_rest bytes_eq]]]]]].
    apply IHcodes in decode_rest as G.
    destruct G as [valid_codes [prefix2 [decode_prefix2 [valid_prefix2 G]]]].
    subst. split.
    + apply Forall_cons. all: assumption.
    + exists (prefix ++ prefix2). repeat split.
      * rewrite utf8_dfa_projects. rewrite decode_prefix, decode_prefix2. reflexivity. assumption.
      * constructor. all: assumption.
      * rewrite app_assoc. reflexivity.
Qed.
```

Da mesma forma, provar que toda sequ√™ncia de bytes √© aceita pelo decodificador n√£o √© complicado, e se reduz a aplicar os lemmas descritos anteriormente.
```coq
Lemma utf8_dfa_input : decoder_input_correct_iff utf8_dfa_decode.
Proof.
  split.
  - intros bytes_valid.
    destruct bytes_valid; unfold utf8_dfa_decode; simpl; unfold next_state, byte_range_dec; lia_simplify; eexists; reflexivity.
  - intros [code decode_bytes].
    apply utf8_dfa_decode_prefix in decode_bytes as G.
    destruct G as [prefix [rest [code_valid [prefix_valid [decode_prefix [decode_rest bytes_eq]]]]]].
    subst.
    apply utf8_dfa_decode_invalid in decode_rest. subst. rewrite app_nil_r in *.
    assumption.
Qed.
```

Infelizmente, a prova de que `utf8_dfa_decode` √© crescente √© complexa, visto que a abordagem for√ßa bruta de desconstruir em todos os casos √© demorada demais. Especificamente, existem 85 maneiras de uma sequ√™ncia de bytes que representa um _code point_ ser aceita por `utf8_dfa_decode`, e dado que essa prova cont√©m duas hip√≥teses que cont√©m `utf8_dfa_decode`, o m√©todo for√ßa bruta resulta em $85 * 85 = 7225$ _goals_ diferentes, n√∫mero grande demais para ser checado em pouco tempo pelo Rocq.

Por causa disso, √© necess√°rio reduzir o n√∫mero de _goals_ antes de tentar prov√°-los. A ideia principal para realizar isso √© notar que quando as listas de bytes de entrada t√™m tamanhos diferentes, ent√£o necessariamente um dos _code points_ de sa√≠da deve ser maior que o outro, visto que os intervalos delimitados pelo formato UTF-8 s√£o disjuntos. Para isso, s√£o provados 4 lemmas que fornecem limites inferiores e superiores para o _code point_ de sa√≠da, bem como o valor n√∫merico um para cada tamanho da lista de entrada.

```coq
Lemma one_byte_bounds : forall byte code,
    valid_codepoint_representation [byte] ->
    utf8_dfa_decode [byte] = ([code], []) ->
    code = byte /\ 0 <= code <= 0x7f.
Proof.

Lemma two_byte_bounds : forall byte1 byte2 code,
    valid_codepoint_representation [byte1; byte2] ->
    utf8_dfa_decode [byte1; byte2] = ([code], []) ->
    code = byte1 mod 32 * 64 + byte2 mod 64
    /\ (0x80 <= code <= 0x7ff).
Proof.

Lemma three_byte_bounds : forall byte1 byte2 byte3 code,
    valid_codepoint_representation [byte1; byte2; byte3] ->
    utf8_dfa_decode [byte1; byte2; byte3] = ([code], []) ->
    code = (byte1 mod 16 * 64 + byte2 mod 64) * 64 + byte3 mod 64 /\
      (0x800 <= code <= 0xffff).
Proof.

Lemma four_byte_bounds : forall byte1 byte2 byte3 byte4 code,
    valid_codepoint_representation [byte1; byte2; byte3; byte4] ->
    utf8_dfa_decode [byte1; byte2; byte3; byte4] = ([code], []) ->
    code = ((byte1 mod 8 * 64 + byte2 mod 64) * 64 + byte3 mod 64) * 64 + byte4 mod 64 /\
      0x1000 <= code <= 0x10ffff.
```

Por fim, a prova √© feita desconstruindo todos os poss√≠veis tamanhos da lista de entrada, de 1 a 4 bytes, para ambas as as hip√≥teses, gerando 16 _goals_ distintos, e depois aplicando o lemma do limite espec√≠fico para o tamanho da lista. A t√°tica `lia` novamente √© suficiente para provar todos os tamanhos.

```coq
Lemma utf8_dfa_increasing : decoder_strictly_increasing utf8_dfa_decode.
```

Finalmente, a prova de que `utf8_dfa_decode` segue a especifica√ß√£o pode ser descrita como a composi√ß√£o dos 5 lemmas provados anteriormente:

```coq
Theorem utf8_decoder_spec_compliant : utf8_decoder_spec utf8_dfa_decode.
Proof.
  split.
  - apply utf8_dfa_nil.
  - apply utf8_dfa_input.
  - apply utf8_dfa_output.
  - apply utf8_dfa_increasing.
  - apply utf8_dfa_projects.
Qed.
```

#pagebreak()

= Conclus√£o e trabalhos futuros





#pagebreak()

#bibliography("references.bib", style: "associacao-brasileira-de-normas-tecnicas")
